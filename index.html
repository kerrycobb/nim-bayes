<!DOCTYPE html>
<html lang="en-us">
<head>
  <title>index.nim</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2280%22>üê≥</text></svg>">
  <meta content="text/html; charset=utf-8" http-equiv="content-type">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <link rel='stylesheet' href='https://unpkg.com/normalize.css/'>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/light.min.css">
  <link rel='stylesheet' href='https://cdn.jsdelivr.net/gh/pietroppeter/nimib/assets/atom-one-light.css'>
  <style>
.nb-box {
  display: flex;
  align-items: center;
  justify-content: space-between;
}
.nb-small {
  font-size: 0.8rem;
}
button.nb-small {
  float: right;
  padding: 2px;
  padding-right: 5px;
  padding-left: 5px;
}
section#source {
  display:none
}
</style>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body,{delimiters:[{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
</head>
<body>
<header>
<div class="nb-box">
  <span><a href=".">üè°</a></span>
  <span><code>index.nim</code></span>
  <span><a href="https://github.com/kerrycobb/nim-bayes"><svg aria-hidden="true" width="1.2em" height="1.2em" style="vertical-align: middle;" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59c.4.07.55-.17.55-.38c0-.19-.01-.82-.01-1.49c-2.01.37-2.53-.49-2.69-.94c-.09-.23-.48-.94-.82-1.13c-.28-.15-.68-.52-.01-.53c.63-.01 1.08.58 1.23.82c.72 1.21 1.87.87 2.33.66c.07-.52.28-.87.51-1.07c-1.78-.2-3.64-.89-3.64-3.95c0-.87.31-1.59.82-2.15c-.08-.2-.36-1.02.08-2.12c0 0 .67-.21 2.2.82c.64-.18 1.32-.27 2-.27c.68 0 1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82c.44 1.1.16 1.92.08 2.12c.51.56.82 1.27.82 2.15c0 3.07-1.87 3.75-3.65 3.95c.29.25.54.73.54 1.48c0 1.07-.01 1.93-.01 2.2c0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z" fill="#000"></path></svg></a></span>
</div>
<hr>
</header><main>
<h1>Bayesian Inference with Linear Model</h1>
<p>At the time of this writing, Nim does not have any libraries for
conducting Bayesian inference. However, it is quite easy for us to
write all of the necessary code ourselves. This is a good exercise for learning
about Bayesian inference and the syntax and speed of Nim make it an excellent
choice for the this. In this tutorial we will walk through
the different parts needed to perform Bayesian inference with a linear model.
We will assume that you have some basic understanding of Bayesian
inference already. There are many excellent introductions available in books
and online.</p>
<p>For a quick refresher, Bayes rule can be written as:</p>
<p>$$ P(\theta|\text{Data}) = \frac{P(\text{Data}|\theta)P(\theta)}{\sum_{\theta}P(\text{Data}|\theta)P(\theta)}$$</p>
<p>Where each of these terms are referred to as:</p>
<p>$$ \text{Posterior} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Marginal Likelihood}}$$</p>
<p>In this tutorial we will condsider a simple linear model
$ y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i} $ where the parameters
$\beta_0$ (y intercept), $\beta_1$ (slope), and $\epsilon$ (random error)
describe the relationship between a predictor variable $x$ and a response
variable $y$, with some unaccounted for residual ranodom error that
is normally distributed.</p>
<p>We well estimate the values of the slope ($\beta_{0}$), the y-intercept
($\beta_{1}$), and the standard deviation of the normally distributed random
error which we will call $\tau$.</p>
<p>We can express this using Bayes rule:</p>
<p>$$ \displaystyle P(\beta_{0}, \beta_{1}, \tau | Data) =
\frac{P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau)}
{\iiint d\beta_{0} d\beta_{1} d\tau P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau)} $$</p>
<h1>Generate Data</h1>
<p>We need some data to work with. Let's simulate data<br />
under the model: $y = 0 + 1x + \epsilon$ where $\epsilon \sim N(0, 1)$ with
$\beta_{0}=0$ $\beta_{1}=1$ and $\tau=1$</p>
<pre><code class="nim hljs"><span class="hljs-keyword">import</span>
  std / sequtils, std / random

<span class="hljs-keyword">import</span>
  stats

<span class="hljs-keyword">var</span>
  n = <span class="hljs-number">100</span>
  b0 = <span class="hljs-number">0.0</span>
  b1 = <span class="hljs-number">1.0</span>
  sd = <span class="hljs-number">1.0</span>
  x = newSeq[<span class="hljs-built_in">float</span>](n)
  y = newSeq[<span class="hljs-built_in">float</span>](n)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; n:
  x[i] = rand(<span class="hljs-number">10.0</span> .. <span class="hljs-number">100.0</span>)
  y[i] = b0 + b1 * x[i] + gauss(<span class="hljs-number">0.0</span>, sd)</code></pre>
<p>We can use <code>ggplotnim</code> to see what these data look like.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">import</span>
  datamancer, ggplotnim

<span class="hljs-keyword">var</span> sim = seqsToDf(x, y)
ggplot(sim, aes(<span class="hljs-string">&quot;x&quot;</span>, <span class="hljs-string">&quot;y&quot;</span>)) + geom_point() + ggsave(<span class="hljs-string">&quot;images/simulated-data.png&quot;</span>)</code></pre>
<pre><samp>StatusSuccess output of write_to_png
</samp></pre>
<figure>
<img src="./images/simulated-data.png" alt="">
<figcaption></figcaption>
</figure>

<h1>Priors</h1>
<p>We need to choose prior probability distributions for each of the parameters
that we are estimating.  Let's use a normal distribution for the priors on
$\beta_{0}$ and $\beta_{1}$ The $\tau$ parameter must be a positive value
greater than 0 so let's use the gamma distribution as the prior on $\tau$.</p>
<p>$$ \beta_{0} \sim Normal(\mu_{0}, \tau_{0})$$
$$ \beta_{1} \sim Normal(\mu_{1}, \tau_{1})$$
$$ \tau \sim Gamma(\alpha_{0}, \beta_{0})$$</p>
<p>Since this is for the purpose of demonstration, let's use very informed
priors so that we can quickly get a good sample from the posterior.</p>
<p>$$ \beta_{0} \sim Normal(0, 1)$$
$$ \beta_{1} \sim Normal(1, 1)$$
$$ \tau \sim Gamma(1, 1)$$</p>
<p>To calculate the prior probability of a proposed parameter value, we will need
the proability density functions for our priors.</p>
<h4>Normal PDF</h4>
<p>$$ p(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^{2}} $$</p>
<h4>Gamma PDF</h4>
<p>$$ p(x) = \frac{1}{\Gamma(k)\theta^{k}} x^{k-1} e^{-\frac{x}{\theta}} $$</p>
<pre><code class="nim hljs"><span class="hljs-keyword">import</span>
  distributions

<span class="hljs-keyword">import</span>
  std / math

<span class="hljs-keyword">proc</span> normalPdf(z, mu, sigma: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
  <span class="hljs-literal">result</span> = pow(<span class="hljs-type">E</span>, (-<span class="hljs-number">0.5</span> * ((z - mu) / sigma) ^ <span class="hljs-number">2</span>)) / (sigma * sqrt(<span class="hljs-number">2.0</span> * <span class="hljs-type">PI</span>))

<span class="hljs-keyword">proc</span> gammaPdf(x, k, theta: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
  <span class="hljs-literal">result</span> = pow(x, k - <span class="hljs-number">1.0</span>) * pow(<span class="hljs-type">E</span>, -(x / theta)) / (gamma(k) * pow(theta, k))</code></pre>
<p>Now that we have probability density functions available, we will be able to
compute a prior probability for a given parameterization of the model.
We will actually be using the $ln$ of the probabilities to
reduce rounding error since these values can be quite small.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> logPrior(b0, b1, sd: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
  <span class="hljs-keyword">let</span>
    b0Prior = ln(normalPdf(b0, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>))
    b1Prior = ln(normalPdf(b1, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>))
    sdPrior = ln(gammaPdf(<span class="hljs-number">1</span> / sd, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>))
  <span class="hljs-literal">result</span> = b0Prior + b1Prior + sdPrior</code></pre>
<h1>Likelihood</h1>
<p>We need to be able to calculate the likelihood of the observed $y_{i}$ values
given the observed $x_{i}$ values and proposed parameter values for $\beta_{0}$,
$\beta_{1}$, and $\tau$.</p>
<p>We can write the model in a slightly different way:</p>
<p>$$\mu = \beta_{0} +\beta_{1} x$$
$$ y \sim N(\mu, \tau) $$</p>
<p>Then to compute the likelihood for a given set of $\beta_{0}$, $\beta_{1}$, $\tau$
parameters and data values $x_{i}$ and $y_{i}$ we use the normal probability
density function which we wrote before. We will again work with the $ln$ of the<br />
likelihood as we did with the priors.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> logLikelihood(x, y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]; b0, b1, sd: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
  <span class="hljs-keyword">var</span> likelihoods = newSeq[<span class="hljs-built_in">float</span>](y.len)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; y.len:
    <span class="hljs-keyword">let</span> pred = b0 + b1 * x[i]
    likelihoods[i] = ln(normalPdf(y[i], pred, sd))
  <span class="hljs-literal">result</span> = sum(likelihoods)</code></pre>
<h1>Posterior</h1>
<p>We cannot analytically solve the posterior probability distribution of our
linear model as the integration of the marginal likelihood is intractable.
But we can approximate it with markov chain monte carlo (mcmc) thanks to this
property of Bayes rule:</p>
<p>$$ \displaystyle P(\beta_{0}, \beta_{1}, \tau | Data) \propto
P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau) $$</p>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> logPosterior(x, y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]; b0, b1, sd: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
  <span class="hljs-keyword">let</span>
    like = logLikelihood(x = x, y = y, b0 = b0, b1 = b1, sd = sd)
    prior = logPrior(b0 = b0, b1 = b1, sd = sd)
  <span class="hljs-literal">result</span> = like + prior</code></pre>
<h1>MCMC</h1>
<p>We will use a Metropolis-Hastings algorithm to approximate the posterior.
The steps of this algorithm are as follows:</p>
<ol>
<li>Choose starting values</li>
<li>Propose new parameter values close to the previous ones.</li>
<li>Accept the proposed parameter value with probability ...</li>
</ol>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> mcmc(x, y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]; nSamples: <span class="hljs-built_in">int</span>; b0, b1, sd, pb0, pb1, psd: <span class="hljs-built_in">float</span>): (
    <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]) =
  <span class="hljs-keyword">var</span>
    b0Samples = newSeq[<span class="hljs-built_in">float</span>](nSamples + <span class="hljs-number">1</span>)
    b1Samples = newSeq[<span class="hljs-built_in">float</span>](nSamples + <span class="hljs-number">1</span>)
    sdSamples = newSeq[<span class="hljs-built_in">float</span>](nSamples + <span class="hljs-number">1</span>)
  b0Samples[<span class="hljs-number">0</span>] = b0
  b1Samples[<span class="hljs-number">0</span>] = b1
  sdSamples[<span class="hljs-number">0</span>] = sd
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span> .. nSamples:
    <span class="hljs-keyword">let</span>
      prevB0 = b0Samples[i - <span class="hljs-number">1</span>]
      prevB1 = b1Samples[i - <span class="hljs-number">1</span>]
      prevSd = sdSamples[i - <span class="hljs-number">1</span>]
      propB0 = gauss(prevB0, pb0)
      propB1 = gauss(prevB1, pb1)
      propSd = gauss(prevSd, psd)
    <span class="hljs-keyword">if</span> propSd &gt; <span class="hljs-number">0.0</span>:
      <span class="hljs-keyword">var</span>
        prevLogPost = logPosterior(x = x, y = y, b0 = prevB0, b1 = prevB1,
                                   sd = prevSd)
        propLogPost = logPosterior(x = x, y = y, b0 = propB0, b1 = propB1,
                                   sd = propSd)
        ratio = exp(propLogPost - prevLogPost)
      <span class="hljs-keyword">if</span> rand(<span class="hljs-number">1.0</span>) &lt; ratio:
        b0Samples[i] = propB0
        b1Samples[i] = propB1
        sdSamples[i] = propSd
      <span class="hljs-keyword">else</span>:
        b0Samples[i] = prevB0
        b1Samples[i] = prevB1
        sdSamples[i] = prevSd
    <span class="hljs-keyword">else</span>:
      b0Samples[i] = prevB0
      b1Samples[i] = prevB1
      sdSamples[i] = prevSd
  <span class="hljs-literal">result</span> = (b0Samples, b1Samples, sdSamples)

<span class="hljs-keyword">var</span>
  nSamples = <span class="hljs-number">100000</span>
  (b0Samples1, b1Samples1, sdSamples1) = mcmc(x, y, nSamples, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>,
      <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>)</code></pre>
<p>Should do another chain with different starting values</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span> (b0Samples2, b1Samples2, sdSamples2) = mcmc(x, y, nSamples, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">0.9</span>,
    <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>)</code></pre>
<h1>Trace plots</h1>
<p>We can get a sense for how well our mcmc performed and therefore gain some<br />
sense for how good our estimates might be by looking at the trace plot which
shows the parameter value store during each step in the mcmc chain. Either
the accepted proposal or the previous one if the a proposal is rejected. Trace
plots can be unreliable for mcmc performance so it is a good<br />
idea to assess this with other methods as well.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span>
  ixs = toSeq(<span class="hljs-number">0</span> .. nSamples)
  df = seqsToDf({<span class="hljs-string">&quot;ixs&quot;</span>: cycle(ixs, <span class="hljs-number">2</span>), <span class="hljs-string">&quot;chain&quot;</span>: concat(repeat(<span class="hljs-number">1</span>, nSamples + <span class="hljs-number">1</span>),
      repeat(<span class="hljs-number">2</span>, nSamples + <span class="hljs-number">1</span>)), <span class="hljs-string">&quot;b0&quot;</span>: concat(b0Samples1, b0Samples2),
                 <span class="hljs-string">&quot;b1&quot;</span>: concat(b1Samples1, b1Samples2),
                 <span class="hljs-string">&quot;sd&quot;</span>: concat(sdSamples1, sdSamples2)})
ggplot(df, aes(x = <span class="hljs-string">&quot;ixs&quot;</span>, y = <span class="hljs-string">&quot;b0&quot;</span>)) + geom_line(aes(color = <span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/samples-b0.png&quot;</span>)
ggplot(df, aes(x = <span class="hljs-string">&quot;ixs&quot;</span>, y = <span class="hljs-string">&quot;b1&quot;</span>)) + geom_line(aes(color = <span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/samples-b1.png&quot;</span>)
ggplot(df, aes(x = <span class="hljs-string">&quot;ixs&quot;</span>, y = <span class="hljs-string">&quot;sd&quot;</span>)) + geom_line(aes(color = <span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/samples-sd.png&quot;</span>)</code></pre>
<pre><samp>INFO: The integer column `ixs` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;ixs&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
INFO: The integer column `ixs` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;ixs&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
INFO: The integer column `ixs` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;ixs&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
</samp></pre>
<figure>
<img src="./images/samples-b0.png" alt="">
<figcaption></figcaption>
</figure>

<figure>
<img src="./images/samples-b1.png" alt="">
<figcaption></figcaption>
</figure>

<figure>
<img src="./images/samples-sd.png" alt="">
<figcaption></figcaption>
</figure>

<h1>Burnin</h1>
<p>Initially the mcmc chain may spend time exploring unlikely regions of
parameter space. We can get a better approximation of the posterior if we
exclude these early steps in the chain. These excluded samples are referred to<br />
as the burnin. A burnin of $10%$ seems to work well with our informative priors
and starting values.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span>
  burnin = (nSamples.<span class="hljs-built_in">float</span> * <span class="hljs-number">0.1</span> + <span class="hljs-number">1</span>).<span class="hljs-built_in">int</span>
  b0Burn = concat(b0Samples1[burnin ..^ <span class="hljs-number">1</span>], b0Samples2[burnin ..^ <span class="hljs-number">1</span>])
  b1Burn = concat(b1Samples1[burnin ..^ <span class="hljs-number">1</span>], b1Samples2[burnin ..^ <span class="hljs-number">1</span>])
  sdBurn = concat(sdSamples1[burnin ..^ <span class="hljs-number">1</span>], sdSamples2[burnin ..^ <span class="hljs-number">1</span>])</code></pre>
<h1>Histograms</h1>
<pre><code class="nim hljs">ixs = toSeq(<span class="hljs-number">0</span> .. nSamples - burnin)
df = seqsToDf({<span class="hljs-string">&quot;ixs&quot;</span>: cycle(ixs, <span class="hljs-number">2</span>),
               <span class="hljs-string">&quot;chain&quot;</span>: concat(repeat(<span class="hljs-number">1</span>, ixs.len), repeat(<span class="hljs-number">2</span>, ixs.len)),
               <span class="hljs-string">&quot;b0&quot;</span>: b0Burn, <span class="hljs-string">&quot;b1&quot;</span>: b1Burn, <span class="hljs-string">&quot;sd&quot;</span>: sdBurn})
ggplot(df, aes(x = <span class="hljs-string">&quot;b0&quot;</span>, fill = <span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position = <span class="hljs-string">&quot;identity&quot;</span>, alpha = some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/hist-b0.png&quot;</span>)
ggplot(df, aes(x = <span class="hljs-string">&quot;b1&quot;</span>, fill = <span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position = <span class="hljs-string">&quot;identity&quot;</span>, alpha = some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/hist-b1.png&quot;</span>)
ggplot(df, aes(x = <span class="hljs-string">&quot;sd&quot;</span>, fill = <span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position = <span class="hljs-string">&quot;identity&quot;</span>, alpha = some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/hist-sd.png&quot;</span>)</code></pre>
<pre><samp>INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
</samp></pre>
<figure>
<img src="./images/hist-b0.png" alt="">
<figcaption></figcaption>
</figure>

<figure>
<img src="./images/hist-b1.png" alt="">
<figcaption></figcaption>
</figure>

<figure>
<img src="./images/hist-sd.png" alt="">
<figcaption></figcaption>
</figure>

<h1>Posterior means</h1>
<p>One way to summarize the estimates from the posterior distribution is to calculate
the mean. Let's see how close these values are to the true values of the parameters.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">import</span>
  stats

<span class="hljs-keyword">var</span>
  meanB0 = mean(b0Burn)
  meanB1 = mean(b1Burn)
  meanSd = mean(sdBurn)
<span class="hljs-keyword">echo</span> meanB0
<span class="hljs-keyword">echo</span> meanB1
<span class="hljs-keyword">echo</span> meanSd</code></pre>
<pre><samp>-0.3239560905850797
1.008189709400152
0.849210855656735
</samp></pre>
<h1>Credible Intervals</h1>
<p>We The means give us a point estimate for our parameter values but they tell us
nothing about the uncertaintly of our estimates. We can get a sense for that by
looking at credible intervals. There are two widely used approaches for this,
equal tailed intervals, and highest density intervals. These will often match
each other closely when the target distribution is unimodal and symetric.
We will calculate the 89% interval for each of these below. Why 89%? Why not?
Credible interval threshold values are completely arbitrary.</p>
<h2>Equal Tailed Interval</h2>
<pre><code class="nim hljs"><span class="hljs-keyword">import</span>
  algorithm

<span class="hljs-keyword">proc</span> quantile(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]; interval: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
  <span class="hljs-keyword">let</span>
    s = sorted(samples, system.cmp[<span class="hljs-built_in">float</span>])
    k = <span class="hljs-built_in">float</span>(s.len - <span class="hljs-number">1</span>) * interval
    f = floor(k)
    c = ceil(k)
  <span class="hljs-keyword">if</span> f == c:
    <span class="hljs-literal">result</span> = s[<span class="hljs-built_in">int</span>(k)]
  <span class="hljs-keyword">else</span>:
    <span class="hljs-keyword">let</span>
      d0 = s[<span class="hljs-built_in">int</span>(f)] * (c - k)
      d1 = s[<span class="hljs-built_in">int</span>(c)] * (k - f)
    <span class="hljs-literal">result</span> = d0 + d1

<span class="hljs-keyword">proc</span> eti(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]; interval: <span class="hljs-built_in">float</span>): (<span class="hljs-built_in">float</span>, <span class="hljs-built_in">float</span>) =
  <span class="hljs-keyword">let</span> p = (<span class="hljs-number">1</span> - interval) / <span class="hljs-number">2</span>
  <span class="hljs-keyword">let</span>
    q0 = quantile(samples, p)
    q1 = quantile(samples, <span class="hljs-number">1</span> - p)
  <span class="hljs-literal">result</span> = (q0, q1)

<span class="hljs-keyword">var</span>
  (b0EtiMin, b0EtiMax) = eti(b0Burn, <span class="hljs-number">0.89</span>)
  (b1EtiMin, b1EtiMax) = eti(b1Burn, <span class="hljs-number">0.89</span>)
  (sdEtiMin, sdEtiMax) = eti(sdBurn, <span class="hljs-number">0.89</span>)
<span class="hljs-keyword">echo</span> b0EtiMin, <span class="hljs-string">&quot; &quot;</span>, b0EtiMax
<span class="hljs-keyword">echo</span> b1EtiMin, <span class="hljs-string">&quot; &quot;</span>, b1EtiMax
<span class="hljs-keyword">echo</span> sdEtiMin, <span class="hljs-string">&quot; &quot;</span>, sdEtiMax</code></pre>
<pre><samp>-0.6170517917322343 -0.008467188688193572
1.002991915598452 1.013250364425417
0.7541564440854457 0.9540643872406819
</samp></pre>
<h2>Highest Density Interval</h2>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> hdi(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]; credMass: <span class="hljs-built_in">float</span>): (<span class="hljs-built_in">float</span>, <span class="hljs-built_in">float</span>) =
  <span class="hljs-keyword">let</span>
    sortedSamples = sorted(samples, system.cmp[<span class="hljs-built_in">float</span>])
    ciIdxInc = <span class="hljs-built_in">int</span>(floor(credMass * <span class="hljs-built_in">float</span>(sortedSamples.len)))
    nCIs = sortedSamples.len - ciIdxInc
  <span class="hljs-keyword">var</span> ciWidth = newSeq[<span class="hljs-built_in">float</span>](nCIs)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; nCIs:
    ciWidth[i] = sortedSamples[i + ciIdxInc] - sortedSamples[i]
  <span class="hljs-keyword">let</span>
    minCiWidthIx = minIndex(ciWidth)
    hdiMin = sortedSamples[minCiWidthIx]
    hdiMax = sortedSamples[minCiWidthIx + ciIdxInc]
  <span class="hljs-literal">result</span> = (hdiMin, hdiMax)

<span class="hljs-keyword">var</span>
  (b0HdiMin, b0HdiMax) = hdi(b0Burn, <span class="hljs-number">0.89</span>)
  (b1HdiMin, b1HdiMax) = hdi(b1Burn, <span class="hljs-number">0.89</span>)
  (sdHdiMin, sdHdiMax) = hdi(sdBurn, <span class="hljs-number">0.89</span>)
<span class="hljs-keyword">echo</span> b0HdiMin, <span class="hljs-string">&quot; &quot;</span>, b0HdiMax
<span class="hljs-keyword">echo</span> b1HdiMin, <span class="hljs-string">&quot; &quot;</span>, b1HdiMax
<span class="hljs-keyword">echo</span> sdHdiMin, <span class="hljs-string">&quot; &quot;</span>, sdHdiMax</code></pre>
<pre><samp>-0.5800420499126808 0.0241196910410927
1.003122485718627 1.013265832517531
0.7433858302364954 0.9376893572599122
</samp></pre>
<h1>Standardize Data</h1>
<p>We might be able to get even better mixing by standardizing the data and
removing correlation between the slope and the intercept.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span>
  stX = newSeq[<span class="hljs-built_in">float</span>](n)
  stY = newSeq[<span class="hljs-built_in">float</span>](n)
  meanX = mean(x)
  sdX = standardDeviation(x)
  meanY = mean(y)
  sdY = standardDeviation(y)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; n:
  stX[i] = (x[i] - meanX) / sdX
  stY[i] = (y[i] - meanY) / sdY</code></pre>
<p>We can see that these data are now centered around zero and have the same scale.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span> standardized = seqsToDf(stX, stY)
ggplot(standardized, aes(<span class="hljs-string">&quot;x&quot;</span>, <span class="hljs-string">&quot;y&quot;</span>)) + geom_point() +
    ggsave(<span class="hljs-string">&quot;images/st-simulated-data.png&quot;</span>)</code></pre>
<pre><samp>StatusSuccess output of write_to_png
</samp></pre>
<figure>
<img src="./images/st-simulated-data.png" alt="">
<figcaption></figcaption>
</figure>

<p>We can run the MCMC as before with some slight changes. Since our data are on a<br />
different scale, the proposals we were making before wont work very well. So
we should make the proposed changes smaller.</p>
<p>We could also have changed our priors since the data are on a different scale
but let's see what happens if we leave them the same.</p>
<pre><code class="nim hljs">(b0Samples1, b1Samples1, sdSamples1) = mcmc(stX, stY, nSamples, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.01</span>,
    <span class="hljs-number">0.01</span>, <span class="hljs-number">0.01</span>)
(b0Samples2, b1Samples2, sdSamples2) = mcmc(stX, stY, nSamples, <span class="hljs-number">0.01</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">1.1</span>,
    <span class="hljs-number">0.01</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.01</span>)</code></pre>
<h3>Convert back to original scale</h3>
<p>To interpret these new estimates we can convert back to the original scale.
$$ \beta_{0} = \zeta_{0} SD_{y} + M_{y} - \zeta_{1} SD_{y} M_{x} / SD_{x} $$<br />
$$ \beta_{1} = \zeta_{1} SD_{y} / SD_{x} $$
TODO: Need to confirm that this is correct:
$$ \tau = \zeta_{\tau} SD_{y} + M_{y} - \zeta_{1} SD_{y} M_{x} / SD_{x} $$</p>
<pre><code class="nim hljs"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; nSamples:
  b0Samples1[i] = b0Samples1[i] * sdY + meanY -
      b1Samples1[i] * sdY * meanX / sdX
  b1Samples1[i] = b1Samples1[i] * sdY / sdX
  sdSamples1[i] = sdSamples1[i] * sdY + meanY -
      b1Samples1[i] * sdY * meanX / sdX
  b0Samples2[i] = b0Samples2[i] * sdY + meanY -
      b1Samples2[i] * sdY * meanX / sdX
  b1Samples2[i] = b1Samples2[i] * sdY / sdX
  sdSamples2[i] = sdSamples2[i] * sdY + meanY -
      b1Samples2[i] * sdY * meanX / sdX</code></pre>
<h1>Traceplots</h1>
<pre><code class="nim hljs">ixs = toSeq(<span class="hljs-number">0</span> .. nSamples)
df = seqsToDf({<span class="hljs-string">&quot;ixs&quot;</span>: cycle(ixs, <span class="hljs-number">2</span>), <span class="hljs-string">&quot;chain&quot;</span>: concat(repeat(<span class="hljs-number">1</span>, nSamples + <span class="hljs-number">1</span>),
    repeat(<span class="hljs-number">2</span>, nSamples + <span class="hljs-number">1</span>)), <span class="hljs-string">&quot;b0&quot;</span>: concat(b0Samples1, b0Samples2),
               <span class="hljs-string">&quot;b1&quot;</span>: concat(b1Samples1, b1Samples2),
               <span class="hljs-string">&quot;sd&quot;</span>: concat(sdSamples1, sdSamples2)})
ggplot(df, aes(x = <span class="hljs-string">&quot;ixs&quot;</span>, y = <span class="hljs-string">&quot;b0&quot;</span>)) + geom_line(aes(color = <span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-samples-b0.png&quot;</span>)
ggplot(df, aes(x = <span class="hljs-string">&quot;ixs&quot;</span>, y = <span class="hljs-string">&quot;b1&quot;</span>)) + geom_line(aes(color = <span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-samples-b1.png&quot;</span>)
ggplot(df, aes(x = <span class="hljs-string">&quot;ixs&quot;</span>, y = <span class="hljs-string">&quot;sd&quot;</span>)) + geom_line(aes(color = <span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-samples-sd.png&quot;</span>)</code></pre>
<pre><samp>INFO: The integer column `ixs` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;ixs&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
INFO: The integer column `ixs` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;ixs&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
INFO: The integer column `ixs` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;ixs&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
</samp></pre>
<figure>
<img src="./images/st-samples-b0.png" alt="">
<figcaption></figcaption>
</figure>

<figure>
<img src="./images/st-samples-b1.png" alt="">
<figcaption></figcaption>
</figure>

<figure>
<img src="./images/st-samples-sd.png" alt="">
<figcaption></figcaption>
</figure>

<h1>Burnin</h1>
<pre><code class="nim hljs">b0Burn = concat(b0Samples1[burnin ..^ <span class="hljs-number">1</span>], b0Samples2[burnin ..^ <span class="hljs-number">1</span>])
b1Burn = concat(b1Samples1[burnin ..^ <span class="hljs-number">1</span>], b1Samples2[burnin ..^ <span class="hljs-number">1</span>])
sdBurn = concat(sdSamples1[burnin ..^ <span class="hljs-number">1</span>], sdSamples2[burnin ..^ <span class="hljs-number">1</span>])</code></pre>
<h1>Histograms</h1>
<pre><code class="nim hljs">ixs = toSeq(<span class="hljs-number">0</span> .. nSamples - burnin)
df = seqsToDf({<span class="hljs-string">&quot;ixs&quot;</span>: cycle(ixs, <span class="hljs-number">2</span>),
               <span class="hljs-string">&quot;chain&quot;</span>: concat(repeat(<span class="hljs-number">1</span>, ixs.len), repeat(<span class="hljs-number">2</span>, ixs.len)),
               <span class="hljs-string">&quot;b0&quot;</span>: b0Burn, <span class="hljs-string">&quot;b1&quot;</span>: b1Burn, <span class="hljs-string">&quot;sd&quot;</span>: sdBurn})
ggplot(df, aes(x = <span class="hljs-string">&quot;b0&quot;</span>, fill = <span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position = <span class="hljs-string">&quot;identity&quot;</span>, alpha = some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-hist-b0.png&quot;</span>)
ggplot(df, aes(x = <span class="hljs-string">&quot;b1&quot;</span>, fill = <span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position = <span class="hljs-string">&quot;identity&quot;</span>, alpha = some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-hist-b1.png&quot;</span>)
ggplot(df, aes(x = <span class="hljs-string">&quot;sd&quot;</span>, fill = <span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position = <span class="hljs-string">&quot;identity&quot;</span>, alpha = some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-hist-sd.png&quot;</span>)</code></pre>
<pre><samp>INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
StatusSuccess output of write_to_png
</samp></pre>
<figure>
<img src="./images/st-hist-b0.png" alt="">
<figcaption></figcaption>
</figure>

<figure>
<img src="./images/st-hist-b1.png" alt="">
<figcaption></figcaption>
</figure>

<figure>
<img src="./images/st-hist-sd.png" alt="">
<figcaption></figcaption>
</figure>

<h1>Posterior Means</h1>
<pre><code class="nim hljs">meanB0 = mean(b0Burn)
meanB1 = mean(b1Burn)
meanSd = mean(sdBurn)
<span class="hljs-keyword">echo</span> meanB0
<span class="hljs-keyword">echo</span> meanB1
<span class="hljs-keyword">echo</span> meanSd</code></pre>
<pre><samp>-0.3551125923794411
1.008640805768852
0.1152425089023909
</samp></pre>
<h1>Equal Tailed Interval</h1>
<pre><code class="nim hljs">(b0EtiMin, b0EtiMax) = eti(b0Burn, <span class="hljs-number">0.89</span>)
(b1EtiMin, b1EtiMax) = eti(b1Burn, <span class="hljs-number">0.89</span>)
(sdEtiMin, sdEtiMax) = eti(sdBurn, <span class="hljs-number">0.89</span>)
<span class="hljs-keyword">echo</span> b0EtiMin, <span class="hljs-string">&quot; &quot;</span>, b0EtiMax
<span class="hljs-keyword">echo</span> b1EtiMin, <span class="hljs-string">&quot; &quot;</span>, b1EtiMax
<span class="hljs-keyword">echo</span> sdEtiMin, <span class="hljs-string">&quot; &quot;</span>, sdEtiMax</code></pre>
<pre><samp>-0.7372544461462738 0.01703279290945403
1.002490005593506 1.0148922615613
-0.2466404130246431 0.4890857276583773
</samp></pre>
<h1>Highest Density Interval</h1>
<pre><code class="nim hljs">(b0HdiMin, b0HdiMax) = hdi(b0Burn, <span class="hljs-number">0.89</span>)
(b1HdiMin, b1HdiMax) = hdi(b1Burn, <span class="hljs-number">0.89</span>)
(sdHdiMin, sdHdiMax) = hdi(sdBurn, <span class="hljs-number">0.89</span>)
<span class="hljs-keyword">echo</span> b0HdiMin, <span class="hljs-string">&quot; &quot;</span>, b0HdiMax
<span class="hljs-keyword">echo</span> b1HdiMin, <span class="hljs-string">&quot; &quot;</span>, b1HdiMax
<span class="hljs-keyword">echo</span> sdHdiMin, <span class="hljs-string">&quot; &quot;</span>, sdHdiMax</code></pre>
<pre><samp>-0.7310572080588926 0.02240049916583331
1.002692477793619 1.015034181780213
-0.2517505112806973 0.4822415687409958
</samp></pre>
<h1>Final Note</h1>
<p>Of course we could have simply and more efficiently done this using least squares regression.
However the Bayesian approach allows us to very easily and intuitively express
uncertainty about our estimates and can be easily extended to much more complex
models for which there are not such simple solutions.</p>

</main>
<footer>
<hr>
<div class="nb-box">
  <span><span class="nb-small">made with <a href="https://pietroppeter.github.io/nimib/">nimib üê≥</a></span></span>
  <span></span>
  <span><button class="nb-small" id="show" onclick="toggleSourceDisplay()">Show Source</button></span>
</div>
</footer>
<section id="source">
<pre><code class="nim hljs"><span class="hljs-keyword">import</span> nimib

nbInit
nbDoc.useLatex

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Bayesian Inference with Linear Model

At the time of this writing, Nim does not have any libraries for
conducting Bayesian inference. However, it is quite easy for us to
write all of the necessary code ourselves. This is a good exercise for learning
about Bayesian inference and the syntax and speed of Nim make it an excellent 
choice for the this. In this tutorial we will walk through 
the different parts needed to perform Bayesian inference with a linear model. 
We will assume that you have some basic understanding of Bayesian
inference already. There are many excellent introductions available in books
and online.

For a quick refresher, Bayes rule can be written as:

$$ P(\theta|\text{Data}) = \frac{P(\text{Data}|\theta)P(\theta)}{\sum_{\theta}P(\text{Data}|\theta)P(\theta)}$$

Where each of these terms are referred to as:

$$ \text{Posterior} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Marginal Likelihood}}$$

In this tutorial we will condsider a simple linear model 
$ y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i} $ where the parameters
$\beta_0$ (y intercept), $\beta_1$ (slope), and $\epsilon$ (random error)
describe the relationship between a predictor variable $x$ and a response
variable $y$, with some unaccounted for residual ranodom error that 
is normally distributed. 

We well estimate the values of the slope ($\beta_{0}$), the y-intercept 
($\beta_{1}$), and the standard deviation of the normally distributed random
error which we will call $\tau$.

We can express this using Bayes rule:

$$ \displaystyle P(\beta_{0}, \beta_{1}, \tau | Data) =
  \frac{P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau)}
  {\iiint d\beta_{0} d\beta_{1} d\tau P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau)} $$


# Generate Data
We need some data to work with. Let's simulate data  
under the model: $y = 0 + 1x + \epsilon$ where $\epsilon \sim N(0, 1)$ with 
$\beta_{0}=0$ $\beta_{1}=1$ and $\tau=1$
&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">import</span> std/sequtils, std/random 
  <span class="hljs-keyword">import</span> stats
  <span class="hljs-keyword">var</span> 
    n = <span class="hljs-number">100</span>
    b0 = <span class="hljs-number">0.0</span>
    b1 = <span class="hljs-number">1.0</span>
    sd = <span class="hljs-number">1.0</span>
    x = newSeq[<span class="hljs-built_in">float</span>](n)
    y = newSeq[<span class="hljs-built_in">float</span>](n)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; n: 
    x[i] = rand(<span class="hljs-number">10.0</span>..<span class="hljs-number">100.0</span>) 
    y[i] = b0 + b1 * x[i] + gauss(<span class="hljs-number">0.0</span>, sd) 
nbText: <span class="hljs-string">md&quot;&quot;&quot;
We can use `ggplotnim` to see what these data look like.
&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">import</span> datamancer, ggplotnim
  <span class="hljs-keyword">var</span> sim = seqsToDf(x, y)
  ggplot(sim, aes(<span class="hljs-string">&quot;x&quot;</span>, <span class="hljs-string">&quot;y&quot;</span>)) +
    geom_point() +
    ggsave(<span class="hljs-string">&quot;images/simulated-data.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/simulated-data.png&quot;</span>)


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Priors
We need to choose prior probability distributions for each of the parameters 
that we are estimating.  Let's use a normal distribution for the priors on 
$\beta_{0}$ and $\beta_{1}$ The $\tau$ parameter must be a positive value 
greater than 0 so let's use the gamma distribution as the prior on $\tau$.

$$ \beta_{0} \sim Normal(\mu_{0}, \tau_{0})$$
$$ \beta_{1} \sim Normal(\mu_{1}, \tau_{1})$$
$$ \tau \sim Gamma(\alpha_{0}, \beta_{0})$$

Since this is for the purpose of demonstration, let's use very informed
priors so that we can quickly get a good sample from the posterior.

$$ \beta_{0} \sim Normal(0, 1)$$
$$ \beta_{1} \sim Normal(1, 1)$$
$$ \tau \sim Gamma(1, 1)$$

To calculate the prior probability of a proposed parameter value, we will need 
the proability density functions for our priors.

#### Normal PDF
$$ p(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^{2}} $$

#### Gamma PDF
$$ p(x) = \frac{1}{\Gamma(k)\theta^{k}} x^{k-1} e^{-\frac{x}{\theta}} $$

&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">import</span> distributions  
  <span class="hljs-keyword">import</span> std/math

  <span class="hljs-keyword">proc</span> normalPdf(z, mu, sigma: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> = 
    <span class="hljs-literal">result</span> = pow(<span class="hljs-type">E</span>, (-<span class="hljs-number">0.5</span> * ((z - mu) / sigma)^<span class="hljs-number">2</span>)) / (sigma * sqrt(<span class="hljs-number">2.0</span> * <span class="hljs-type">PI</span>))

  <span class="hljs-keyword">proc</span> gammaPdf(x, k, theta: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> = 
    <span class="hljs-literal">result</span> = pow(x, k - <span class="hljs-number">1.0</span>) * pow(<span class="hljs-type">E</span>, -(x / theta)) / (gamma(k) * pow(theta, k))

nbText: <span class="hljs-string">md&quot;&quot;&quot;

Now that we have probability density functions available, we will be able to
compute a prior probability for a given parameterization of the model.
We will actually be using the $ln$ of the probabilities to
reduce rounding error since these values can be quite small.
&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">proc</span> logPrior(b0, b1, sd: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> = 
    <span class="hljs-keyword">let</span> 
      b0Prior = ln(normalPdf(b0, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>))
      b1Prior = ln(normalPdf(b1, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>))
      sdPrior = ln(gammaPdf(<span class="hljs-number">1</span>/sd, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>))
    <span class="hljs-literal">result</span> = b0Prior + b1Prior + sdPrior


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Likelihood
We need to be able to calculate the likelihood of the observed $y_{i}$ values
given the observed $x_{i}$ values and proposed parameter values for $\beta_{0}$, 
$\beta_{1}$, and $\tau$. 

We can write the model in a slightly different way:   

$$\mu = \beta_{0} +\beta_{1} x$$
$$ y \sim N(\mu, \tau) $$

Then to compute the likelihood for a given set of $\beta_{0}$, $\beta_{1}$, $\tau$ 
parameters and data values $x_{i}$ and $y_{i}$ we use the normal probability 
density function which we wrote before. We will again work with the $ln$ of the  
likelihood as we did with the priors.

&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">proc</span> logLikelihood(x, y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], b0, b1, sd: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> = 
    <span class="hljs-keyword">var</span> likelihoods = newSeq[<span class="hljs-built_in">float</span>](y.len) 
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; y.len: 
      <span class="hljs-keyword">let</span> pred = b0 + b1 * x[i]
      likelihoods[i] = ln(normalPdf(y[i], pred, sd))
    <span class="hljs-literal">result</span> = sum(likelihoods) 


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Posterior
We cannot analytically solve the posterior probability distribution of our
linear model as the integration of the marginal likelihood is intractable. 
But we can approximate it with markov chain monte carlo (mcmc) thanks to this 
property of Bayes rule:

$$ \displaystyle P(\beta_{0}, \beta_{1}, \tau | Data) \propto
  P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau) $$

&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">proc</span> logPosterior(x, y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], b0, b1, sd: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> = 
    <span class="hljs-keyword">let</span> 
      like = logLikelihood(x=x, y=y, b0=b0, b1=b1, sd=sd)
      prior = logPrior(b0=b0, b1=b1, sd=sd)
    <span class="hljs-literal">result</span> = like + prior


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# MCMC
We will use a Metropolis-Hastings algorithm to approximate the posterior. 
The steps of this algorithm are as follows:
1) Choose starting values
2) Propose new parameter values close to the previous ones.
3) Accept the proposed parameter value with probability ...
&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">proc</span> mcmc(x, y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], nSamples: <span class="hljs-built_in">int</span>, b0, b1, sd, pb0, pb1, psd: <span class="hljs-built_in">float</span>): 
      (<span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]) =  
    <span class="hljs-keyword">var</span> 
      b0Samples = newSeq[<span class="hljs-built_in">float</span>](nSamples+<span class="hljs-number">1</span>) 
      b1Samples = newSeq[<span class="hljs-built_in">float</span>](nSamples+<span class="hljs-number">1</span>) 
      sdSamples = newSeq[<span class="hljs-built_in">float</span>](nSamples+<span class="hljs-number">1</span>) 
  
    b0Samples[<span class="hljs-number">0</span>] = b0  
    b1Samples[<span class="hljs-number">0</span>] = b1 
    sdSamples[<span class="hljs-number">0</span>] = sd 
    
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1.</span>.nSamples:
      <span class="hljs-keyword">let</span> 
        prevB0 = b0Samples[i-<span class="hljs-number">1</span>]
        prevB1 = b1Samples[i-<span class="hljs-number">1</span>]
        prevSd = sdSamples[i-<span class="hljs-number">1</span>]
        propB0 = gauss(prevB0, pb0) 
        propB1 = gauss(prevB1, pb1)
        propSd = gauss(prevSd, psd)
      <span class="hljs-keyword">if</span> propSd &gt; <span class="hljs-number">0.0</span>:
        <span class="hljs-keyword">var</span>
          prevLogPost = logPosterior(x=x, y=y, b0=prevB0, b1=prevB1, sd=prevSd) 
          propLogPost = logPosterior(x=x, y=y, b0=propB0, b1=propB1, sd=propSd) 
          ratio = exp(propLogPost - prevLogPost)  
        <span class="hljs-keyword">if</span> rand(<span class="hljs-number">1.0</span>) &lt; ratio:
          b0Samples[i] = propB0  
          b1Samples[i] = propB1  
          sdSamples[i] = propSd  
        <span class="hljs-keyword">else</span>: 
          b0Samples[i] = prevB0  
          b1Samples[i] = prevB1  
          sdSamples[i] = prevSd  
      <span class="hljs-keyword">else</span>:
        b0Samples[i] = prevB0  
        b1Samples[i] = prevB1  
        sdSamples[i] = prevSd  
    <span class="hljs-literal">result</span> = (b0Samples, b1Samples, sdSamples)
  <span class="hljs-keyword">var</span>
    nSamples = <span class="hljs-number">100000</span>
    (b0Samples1, b1Samples1, sdSamples1) = mcmc(x, y, nSamples, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>, 
                                             <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>) 
  
nbText: <span class="hljs-string">md&quot;&quot;&quot;
Should do another chain with different starting values
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">var</span>
    (b0Samples2, b1Samples2, sdSamples2) = mcmc(x, y, nSamples, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">0.9</span>, 
                                                <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>)


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Trace plots 
We can get a sense for how well our mcmc performed and therefore gain some  
sense for how good our estimates might be by looking at the trace plot which 
shows the parameter value store during each step in the mcmc chain. Either 
the accepted proposal or the previous one if the a proposal is rejected. Trace
plots can be unreliable for mcmc performance so it is a good  
idea to assess this with other methods as well. 
&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">var</span> 
    ixs = toSeq(<span class="hljs-number">0</span> .. nSamples)
    df = seqsToDf({
      <span class="hljs-string">&quot;ixs&quot;</span>: cycle(ixs, <span class="hljs-number">2</span>), 
      <span class="hljs-string">&quot;chain&quot;</span>: concat(repeat(<span class="hljs-number">1</span>, nSamples + <span class="hljs-number">1</span>), repeat(<span class="hljs-number">2</span>, nSamples + <span class="hljs-number">1</span>)), 
      <span class="hljs-string">&quot;b0&quot;</span>: concat(b0Samples1, b0Samples2),
      <span class="hljs-string">&quot;b1&quot;</span>: concat(b1Samples1, b1Samples2),
      <span class="hljs-string">&quot;sd&quot;</span>: concat(sdSamples1, sdSamples2)})
  ggplot(df, aes(x=<span class="hljs-string">&quot;ixs&quot;</span>, y=<span class="hljs-string">&quot;b0&quot;</span>)) + 
    geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/samples-b0.png&quot;</span>)
  
  ggplot(df, aes(x=<span class="hljs-string">&quot;ixs&quot;</span>, y=<span class="hljs-string">&quot;b1&quot;</span>)) + 
    geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/samples-b1.png&quot;</span>)
  
  ggplot(df, aes(x=<span class="hljs-string">&quot;ixs&quot;</span>, y=<span class="hljs-string">&quot;sd&quot;</span>)) + 
    geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/samples-sd.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/samples-b0.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/samples-b1.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/samples-sd.png&quot;</span>)



nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Burnin
Initially the mcmc chain may spend time exploring unlikely regions of 
parameter space. We can get a better approximation of the posterior if we 
exclude these early steps in the chain. These excluded samples are referred to   
as the burnin. A burnin of $10%$ seems to work well with our informative priors 
and starting values. 
&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">var</span> 
    burnin = (nSamples.<span class="hljs-built_in">float</span> * <span class="hljs-number">0.1</span> + <span class="hljs-number">1</span>).<span class="hljs-built_in">int</span>
    b0Burn = concat(b0Samples1[burnin..^<span class="hljs-number">1</span>], b0Samples2[burnin..^<span class="hljs-number">1</span>])
    b1Burn = concat(b1Samples1[burnin..^<span class="hljs-number">1</span>], b1Samples2[burnin..^<span class="hljs-number">1</span>])
    sdBurn = concat(sdSamples1[burnin..^<span class="hljs-number">1</span>], sdSamples2[burnin..^<span class="hljs-number">1</span>])

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Histograms 
&quot;&quot;&quot;</span>
nbCode:
  ixs = toSeq(<span class="hljs-number">0</span> .. nSamples - burnin)
  df = seqsToDf({
    <span class="hljs-string">&quot;ixs&quot;</span>: cycle(ixs, <span class="hljs-number">2</span>), 
    <span class="hljs-string">&quot;chain&quot;</span>: concat(repeat(<span class="hljs-number">1</span>, ixs.len), repeat(<span class="hljs-number">2</span>, ixs.len)), 
    <span class="hljs-string">&quot;b0&quot;</span>: b0Burn,
    <span class="hljs-string">&quot;b1&quot;</span>: b1Burn,
    <span class="hljs-string">&quot;sd&quot;</span>: sdBurn})
  ggplot(df, aes(x=<span class="hljs-string">&quot;b0&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/hist-b0.png&quot;</span>)
  
  ggplot(df, aes(x=<span class="hljs-string">&quot;b1&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/hist-b1.png&quot;</span>)
  
  ggplot(df, aes(x=<span class="hljs-string">&quot;sd&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/hist-sd.png&quot;</span>)

nbImage(<span class="hljs-string">&quot;images/hist-b0.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/hist-b1.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/hist-sd.png&quot;</span>)


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Posterior means
One way to summarize the estimates from the posterior distribution is to calculate
the mean. Let's see how close these values are to the true values of the parameters. 
&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">import</span> stats
  <span class="hljs-keyword">var</span> 
    meanB0 = mean(b0Burn)
    meanB1 = mean(b1Burn)
    meanSd = mean(sdBurn)
  <span class="hljs-keyword">echo</span> meanB0
  <span class="hljs-keyword">echo</span> meanB1
  <span class="hljs-keyword">echo</span> meanSd


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Credible Intervals
We The means give us a point estimate for our parameter values but they tell us
nothing about the uncertaintly of our estimates. We can get a sense for that by
looking at credible intervals. There are two widely used approaches for this,
equal tailed intervals, and highest density intervals. These will often match 
each other closely when the target distribution is unimodal and symetric. 
We will calculate the 89% interval for each of these below. Why 89%? Why not? 
Credible interval threshold values are completely arbitrary.

## Equal Tailed Interval 
&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">import</span> algorithm

  <span class="hljs-keyword">proc</span> quantile(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], interval: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> = 
    <span class="hljs-keyword">let</span> 
      s = sorted(samples, system.cmp[<span class="hljs-built_in">float</span>])
      k = <span class="hljs-built_in">float</span>(s.len - <span class="hljs-number">1</span>) * interval 
      f = floor(k)
      c = ceil(k)
    <span class="hljs-keyword">if</span> f == c: 
      <span class="hljs-literal">result</span> = s[<span class="hljs-built_in">int</span>(k)] 
    <span class="hljs-keyword">else</span>:
      <span class="hljs-keyword">let</span> 
        d0 = s[<span class="hljs-built_in">int</span>(f)] * (c - k)
        d1 = s[<span class="hljs-built_in">int</span>(c)] * (k - f)
      <span class="hljs-literal">result</span> = d0 + d1

  <span class="hljs-keyword">proc</span> eti(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], interval: <span class="hljs-built_in">float</span>): (<span class="hljs-built_in">float</span>, <span class="hljs-built_in">float</span>) =  
    <span class="hljs-keyword">let</span>    
      p = (<span class="hljs-number">1</span> - interval) / <span class="hljs-number">2</span>
    <span class="hljs-keyword">let</span>
      q0 = quantile(samples, p)
      q1 = quantile(samples, <span class="hljs-number">1</span> - p)
    <span class="hljs-literal">result</span> = (q0, q1)
  
  <span class="hljs-keyword">var</span> 
    (b0EtiMin, b0EtiMax) = eti(b0Burn, <span class="hljs-number">0.89</span>)
    (b1EtiMin, b1EtiMax) = eti(b1Burn, <span class="hljs-number">0.89</span>)
    (sdEtiMin, sdEtiMax) = eti(sdBurn, <span class="hljs-number">0.89</span>)
  <span class="hljs-keyword">echo</span> b0EtiMin, <span class="hljs-string">&quot; &quot;</span>, b0EtiMax
  <span class="hljs-keyword">echo</span> b1EtiMin, <span class="hljs-string">&quot; &quot;</span>, b1EtiMax
  <span class="hljs-keyword">echo</span> sdEtiMin, <span class="hljs-string">&quot; &quot;</span>, sdEtiMax


nbText: <span class="hljs-string">md&quot;&quot;&quot;
## Highest Density Interval
&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">proc</span> hdi(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], credMass: <span class="hljs-built_in">float</span>): (<span class="hljs-built_in">float</span>, <span class="hljs-built_in">float</span>) =  
    <span class="hljs-keyword">let</span> 
      sortedSamples = sorted(samples, system.cmp[<span class="hljs-built_in">float</span>])
      ciIdxInc = <span class="hljs-built_in">int</span>(floor(credMass * <span class="hljs-built_in">float</span>(sortedSamples.len)))
      nCIs = sortedSamples.len - ciIdxInc
    <span class="hljs-keyword">var</span> ciWidth = newSeq[<span class="hljs-built_in">float</span>](nCIs)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0.</span>.&lt;nCIs:
      ciWidth[i] = sortedSamples[i + ciIdxInc] - sortedSamples[i]
    <span class="hljs-keyword">let</span>
      minCiWidthIx = minIndex(ciWidth)
      hdiMin = sortedSamples[minCiWidthIx]
      hdiMax = sortedSamples[minCiWidthIx + ciIdxInc]
    <span class="hljs-literal">result</span> = (hdiMin, hdiMax)

  <span class="hljs-keyword">var</span> 
    (b0HdiMin, b0HdiMax) = hdi(b0Burn, <span class="hljs-number">0.89</span>)
    (b1HdiMin, b1HdiMax) = hdi(b1Burn, <span class="hljs-number">0.89</span>)
    (sdHdiMin, sdHdiMax) = hdi(sdBurn, <span class="hljs-number">0.89</span>)
  <span class="hljs-keyword">echo</span> b0HdiMin, <span class="hljs-string">&quot; &quot;</span>, b0HdiMax
  <span class="hljs-keyword">echo</span> b1HdiMin, <span class="hljs-string">&quot; &quot;</span>, b1HdiMax
  <span class="hljs-keyword">echo</span> sdHdiMin, <span class="hljs-string">&quot; &quot;</span>, sdHdiMax


<span class="hljs-comment"># # TODO: Add support interval calculation</span>


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Standardize Data
We might be able to get even better mixing by standardizing the data and 
removing correlation between the slope and the intercept.
&quot;&quot;&quot;</span>
nbCode: 
  <span class="hljs-keyword">var</span> 
    stX = newSeq[<span class="hljs-built_in">float</span>](n)
    stY = newSeq[<span class="hljs-built_in">float</span>](n)
    meanX = mean(x) 
    sdX = standardDeviation(x)
    meanY = mean(y) 
    sdY = standardDeviation(y)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; n:
    stX[i] = (x[i] - meanX) / sdX 
    stY[i] = (y[i] - meanY) / sdY


nbText: <span class="hljs-string">md&quot;&quot;&quot;
We can see that these data are now centered around zero and have the same scale.
&quot;&quot;&quot;</span>
nbCode:
  <span class="hljs-keyword">var</span> standardized = seqsToDf(stX, stY)
  ggplot(standardized, aes(<span class="hljs-string">&quot;x&quot;</span>, <span class="hljs-string">&quot;y&quot;</span>)) +
    geom_point() +
    ggsave(<span class="hljs-string">&quot;images/st-simulated-data.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/st-simulated-data.png&quot;</span>)


nbText: <span class="hljs-string">md&quot;&quot;&quot;
We can run the MCMC as before with some slight changes. Since our data are on a  
different scale, the proposals we were making before wont work very well. So
we should make the proposed changes smaller.

We could also have changed our priors since the data are on a different scale
but let's see what happens if we leave them the same.
&quot;&quot;&quot;</span>
nbCode:
  (b0Samples1, b1Samples1, sdSamples1) = mcmc(stX, stY, nSamples, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.01</span>) 
  (b0Samples2, b1Samples2, sdSamples2) = mcmc(stX, stY, nSamples, <span class="hljs-number">0.01</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.01</span>) 


nbText: <span class="hljs-string">md&quot;&quot;&quot; 
### Convert back to original scale
To interpret these new estimates we can convert back to the original scale.
$$ \beta_{0} = \zeta_{0} SD_{y} + M_{y} - \zeta_{1} SD_{y} M_{x} / SD_{x} $$  
$$ \beta_{1} = \zeta_{1} SD_{y} / SD_{x} $$ 
TODO: Need to confirm that this is correct:
$$ \tau = \zeta_{\tau} SD_{y} + M_{y} - \zeta_{1} SD_{y} M_{x} / SD_{x} $$  
&quot;&quot;&quot;</span>
nbCode: 
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; nSamples:
    b0Samples1[i] = b0Samples1[i] * sdY + meanY - b1Samples1[i] * sdY * meanX / sdX
    b1Samples1[i] = b1Samples1[i] * sdY / sdX 
    sdSamples1[i] = sdSamples1[i] * sdY + meanY - b1Samples1[i] * sdY * meanX / sdX
    b0Samples2[i] = b0Samples2[i] * sdY + meanY - b1Samples2[i] * sdY * meanX / sdX
    b1Samples2[i] = b1Samples2[i] * sdY / sdX 
    sdSamples2[i] = sdSamples2[i] * sdY + meanY - b1Samples2[i] * sdY * meanX / sdX


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Traceplots
&quot;&quot;&quot;</span>
nbCode:
  ixs = toSeq(<span class="hljs-number">0</span> .. nSamples)
  df = seqsToDf({
    <span class="hljs-string">&quot;ixs&quot;</span>: cycle(ixs, <span class="hljs-number">2</span>), 
    <span class="hljs-string">&quot;chain&quot;</span>: concat(repeat(<span class="hljs-number">1</span>, nSamples + <span class="hljs-number">1</span>), repeat(<span class="hljs-number">2</span>, nSamples + <span class="hljs-number">1</span>)), 
    <span class="hljs-string">&quot;b0&quot;</span>: concat(b0Samples1, b0Samples2),
    <span class="hljs-string">&quot;b1&quot;</span>: concat(b1Samples1, b1Samples2),
    <span class="hljs-string">&quot;sd&quot;</span>: concat(sdSamples1, sdSamples2)})
  ggplot(df, aes(x=<span class="hljs-string">&quot;ixs&quot;</span>, y=<span class="hljs-string">&quot;b0&quot;</span>)) + 
    geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-samples-b0.png&quot;</span>)
  
  ggplot(df, aes(x=<span class="hljs-string">&quot;ixs&quot;</span>, y=<span class="hljs-string">&quot;b1&quot;</span>)) + 
    geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-samples-b1.png&quot;</span>)
  
  ggplot(df, aes(x=<span class="hljs-string">&quot;ixs&quot;</span>, y=<span class="hljs-string">&quot;sd&quot;</span>)) + 
    geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-samples-sd.png&quot;</span>)

nbImage(<span class="hljs-string">&quot;images/st-samples-b0.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/st-samples-b1.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/st-samples-sd.png&quot;</span>)


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Burnin
&quot;&quot;&quot;</span>
nbCode:
  b0Burn = concat(b0Samples1[burnin..^<span class="hljs-number">1</span>], b0Samples2[burnin..^<span class="hljs-number">1</span>])
  b1Burn = concat(b1Samples1[burnin..^<span class="hljs-number">1</span>], b1Samples2[burnin..^<span class="hljs-number">1</span>])
  sdBurn = concat(sdSamples1[burnin..^<span class="hljs-number">1</span>], sdSamples2[burnin..^<span class="hljs-number">1</span>])


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Histograms
&quot;&quot;&quot;</span>
nbCode:
  ixs = toSeq(<span class="hljs-number">0</span> .. nSamples - burnin)
  df = seqsToDf({
    <span class="hljs-string">&quot;ixs&quot;</span>: cycle(ixs, <span class="hljs-number">2</span>), 
    <span class="hljs-string">&quot;chain&quot;</span>: concat(repeat(<span class="hljs-number">1</span>, ixs.len), repeat(<span class="hljs-number">2</span>, ixs.len)), 
    <span class="hljs-string">&quot;b0&quot;</span>: b0Burn,
    <span class="hljs-string">&quot;b1&quot;</span>: b1Burn,
    <span class="hljs-string">&quot;sd&quot;</span>: sdBurn})
  ggplot(df, aes(x=<span class="hljs-string">&quot;b0&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-hist-b0.png&quot;</span>)
  
  ggplot(df, aes(x=<span class="hljs-string">&quot;b1&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-hist-b1.png&quot;</span>)
  
  ggplot(df, aes(x=<span class="hljs-string">&quot;sd&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">&quot;images/st-hist-sd.png&quot;</span>)

nbImage(<span class="hljs-string">&quot;images/st-hist-b0.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/st-hist-b1.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/st-hist-sd.png&quot;</span>)


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Posterior Means
&quot;&quot;&quot;</span>
nbCode:
  meanB0 = mean(b0Burn)
  meanB1 = mean(b1Burn)
  meanSd = mean(sdBurn)
  <span class="hljs-keyword">echo</span> meanB0
  <span class="hljs-keyword">echo</span> meanB1
  <span class="hljs-keyword">echo</span> meanSd


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Equal Tailed Interval
&quot;&quot;&quot;</span>
nbCode:
  (b0EtiMin, b0EtiMax) = eti(b0Burn, <span class="hljs-number">0.89</span>)
  (b1EtiMin, b1EtiMax) = eti(b1Burn, <span class="hljs-number">0.89</span>)
  (sdEtiMin, sdEtiMax) = eti(sdBurn, <span class="hljs-number">0.89</span>)
  <span class="hljs-keyword">echo</span> b0EtiMin, <span class="hljs-string">&quot; &quot;</span>, b0EtiMax
  <span class="hljs-keyword">echo</span> b1EtiMin, <span class="hljs-string">&quot; &quot;</span>, b1EtiMax
  <span class="hljs-keyword">echo</span> sdEtiMin, <span class="hljs-string">&quot; &quot;</span>, sdEtiMax


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Highest Density Interval
&quot;&quot;&quot;</span>
nbCode:
  (b0HdiMin, b0HdiMax) = hdi(b0Burn, <span class="hljs-number">0.89</span>)
  (b1HdiMin, b1HdiMax) = hdi(b1Burn, <span class="hljs-number">0.89</span>)
  (sdHdiMin, sdHdiMax) = hdi(sdBurn, <span class="hljs-number">0.89</span>)
  <span class="hljs-keyword">echo</span> b0HdiMin, <span class="hljs-string">&quot; &quot;</span>, b0HdiMax
  <span class="hljs-keyword">echo</span> b1HdiMin, <span class="hljs-string">&quot; &quot;</span>, b1HdiMax
  <span class="hljs-keyword">echo</span> sdHdiMin, <span class="hljs-string">&quot; &quot;</span>, sdHdiMax


nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Final Note 
Of course we could have simply and more efficiently done this using least squares regression.
However the Bayesian approach allows us to very easily and intuitively express
uncertainty about our estimates and can be easily extended to much more complex 
models for which there are not such simple solutions.
&quot;&quot;&quot;</span>

nbSave</code></pre>
</section><script>
function toggleSourceDisplay() {
  var btn = document.getElementById("show")
  var source = document.getElementById("source");
  if (btn.innerHTML=="Show Source") {
    btn.innerHTML = "Hide Source";
    source.style.display = "block";
  } else {
    btn.innerHTML = "Show Source";
    source.style.display = "none";
  }
}
</script></body>
</html>