<!DOCTYPE html>
<html lang="en-us">
<head>
  <title>index.nim</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2280%22>üê≥</text></svg>">
  <meta content="text/html; charset=utf-8" http-equiv="content-type">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <link rel='stylesheet' href='https://unpkg.com/normalize.css'>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/light.min.css">
  <link rel='stylesheet' href='https://cdn.jsdelivr.net/gh/pietroppeter/nimib/assets/atom-one-light.css'>
  <style>
.nb-box {
  display: flex;
  align-items: center;
  justify-content: space-between;
}
.nb-small {
  font-size: 0.8rem;
}
button.nb-small {
  float: right;
  padding: 2px;
  padding-right: 5px;
  padding-left: 5px;
}
section#source {
  display:none
}

.nb-output {
  line-height: 1.15;
}
</style>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body,{delimiters:[{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>
</head>
<body>
<header>
<div class="nb-box">
  <span><a href=".">üè°</a></span>
  <span><code>index.nim</code></span>
  <span><a href="https://github.com/kerrycobb/nim-bayes"><svg aria-hidden="true" width="1.2em" height="1.2em" style="vertical-align: middle;" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59c.4.07.55-.17.55-.38c0-.19-.01-.82-.01-1.49c-2.01.37-2.53-.49-2.69-.94c-.09-.23-.48-.94-.82-1.13c-.28-.15-.68-.52-.01-.53c.63-.01 1.08.58 1.23.82c.72 1.21 1.87.87 2.33.66c.07-.52.28-.87.51-1.07c-1.78-.2-3.64-.89-3.64-3.95c0-.87.31-1.59.82-2.15c-.08-.2-.36-1.02.08-2.12c0 0 .67-.21 2.2.82c.64-.18 1.32-.27 2-.27c.68 0 1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82c.44 1.1.16 1.92.08 2.12c.51.56.82 1.27.82 2.15c0 3.07-1.87 3.75-3.65 3.95c.29.25.54.73.54 1.48c0 1.07-.01 1.93-.01 2.2c0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z" fill="#000"></path></svg></a></span>
</div>
<hr>
</header><main>
<h1>Bayesian Inference with Linear Model</h1>
<p>At the time of this writing, Nim does not have any libraries for
conducting Bayesian inference. However, it is quite easy for us to
write all of the necessary code ourselves. This is a good exercise for learning
about Bayesian inference and the syntax and speed of Nim make it an excellent
choice for the this. In this tutorial we will walk through
the different parts needed to perform Bayesian inference with a linear model.
We will assume that you have some basic understanding of Bayesian
inference already. There are many excellent introductions available in books
and online.</p>
<p>Bayes theorem states:</p>
<p>$$ P(\theta|\text{Data}) = \frac{P(\text{Data}|\theta)P(\theta)}{P(\text{Data})}$$</p>
<p>Each of these terms are referred to as:</p>
<p>$$ \text{Posterior} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Marginal Likelihood}}$$</p>
<p>A marginal probability $P(\text{Data}) $ can be discrete probability distribution
where $P(\text{Data}) = \sum_{\theta}P(\text{Data}|\theta)P(\theta)$ so the posterior
probability distribution is:</p>
<p>$$ P(\theta|\text{Data}) = \frac{P(\text{Data}|\theta)P(\theta)}{\sum_{\theta}P(\text{Data}|\theta)P(\theta)}$$</p>
<p>Or a marginal probability can be a continuous probability distribution
where $P(\text{Data}) = \int d\theta P(\text{Data}|\theta)P(\theta)$ so the posterior
probability distribution is:</p>
<p>$$ P(\theta|\text{Data}) = \frac{P(\text{Data}|\theta)P(\theta)}{\int d\theta P(\text{Data}|\theta)P(\theta)}$$</p>
<p>In this tutorial we will condsider a simple linear model
$ y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i} $ where $\epsilon \sim N(0, \tau)$.
Under this model, the parameters $\beta_0$ (y intercept) and $\beta_1$ (slope),
describe the relationship between a
predictor variable $x$ and a response variable $y$, with some unaccounted for
residual random error ($\epsilon$) which is normally distributed with a mean of $0$ and
standard deviation $\tau$.</p>
<p>We will estimate the values of the slope ($\beta_{0}$), the y-intercept
($\beta_{1}$), and the standard deviation of the residual random error ($\tau$)
from observed $x$ and $y$ data.</p>
<p>Expressing this with Bayes rule gives us:</p>
<p>$$ \displaystyle P(\beta_{0}, \beta_{1}, \tau | Data) =
\frac{P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau)}
{\iiint d\beta_{0} d\beta_{1} d\tau P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau)} $$</p>
<h1>Generate Data</h1>
<p>We need some data to work with. Let's simulate data
under the model wth $\beta_{0}=0$, $\beta_{1}=1$, and $\tau=1$:</p>
<p>$ y = 0 + 1x + \epsilon$ where $\epsilon \sim N(0, 1) $</p>
<pre><code class="nim hljs"><span class="hljs-keyword">import</span> std/sequtils, std/random, std/stats
<span class="hljs-keyword">var</span>
  n = <span class="hljs-number">100</span>
  b0 = <span class="hljs-number">0.0</span>
  b1 = <span class="hljs-number">1.0</span>
  sd = <span class="hljs-number">1.0</span>
  x = newSeq[<span class="hljs-built_in">float</span>](n)
  y = newSeq[<span class="hljs-built_in">float</span>](n)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; n:
  x[i] = rand(<span class="hljs-number">0.0</span>..<span class="hljs-number">100.0</span>)
  y[i] = b0 + (b1 * x[i]) + gauss(<span class="hljs-number">0.0</span>, sd)</code></pre>
<p>We can use <code>ggplotnim</code> to see what these data look like.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">import</span> datamancer, ggplotnim
<span class="hljs-keyword">var</span> sim = toDf(x, y)
ggplot(sim, aes(<span class="hljs-string">&quot;x&quot;</span>, <span class="hljs-string">&quot;y&quot;</span>)) +
  geom_point() +
  ggsave(<span class="hljs-string">&quot;images/simulated-data.png&quot;</span>)</code></pre>
<figure>
<img src="./images/simulated-data.png" alt="">
<figcaption></figcaption>
</figure>
<h1>Priors</h1>
<p>We need to choose prior probability distributions for each of the model parameters
that we are estimating.  Let's use a normal distribution for the priors on
$\beta_{0}$ and $\beta_{1}$. The $\tau$ parameter must be a positive value
greater than 0 so let's use the gamma distribution as the prior on $\tau$.</p>
<p>$$ \beta_{0} \sim Normal(\mu_{0}, \sigma^{2})$$
$$ \beta_{1} \sim Normal(\mu_{1}, \sigma^{2})$$
$$ \tau \sim Gamma(\kappa_{0}, \theta_{0})$$</p>
<p>To calculate the prior probabilities of proposed model parameter values, we will need
the proability density functions for these distributions.</p>
<h4>Normal PDF</h4>
<p>$$ p(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^{2}} $$</p>
<h4>Gamma PDF</h4>
<p>$$ p(x) = \frac{1}{\Gamma(k)\theta^{k}} x^{k-1} e^{-\frac{x}{\theta}} $$</p>
<pre><code class="nim hljs"><span class="hljs-keyword">import</span> std/math

<span class="hljs-keyword">proc</span> normalPdf(x, m, s: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
  <span class="hljs-literal">result</span> = <span class="hljs-type">E</span>.pow((-<span class="hljs-number">0.5</span> * ((x - m) / s)^<span class="hljs-number">2</span>)) / (s * sqrt(<span class="hljs-number">2.0</span> * <span class="hljs-type">PI</span>))

<span class="hljs-keyword">proc</span> gammaPdf(x, k, t: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
  <span class="hljs-literal">result</span> = x.pow(k - <span class="hljs-number">1.0</span>) * <span class="hljs-type">E</span>.pow(-(x / t)) / (gamma(k) * t.pow(k))</code></pre>
<p>We now need to decide how to parameterize these prior probability densities.
Since this is for the purpose of demonstration, let's use very informed
priors so that we can quickly get a good sample from the posterior.</p>
<p>$$ \beta_{0} \sim Normal(0, 1)$$
$$ \beta_{1} \sim Normal(1, 1)$$
$$ \tau \sim Gamma(1, 1)$$</p>
<p>In a real analysis, the priors should encompass all values that we consider possible and
it may also be a good idea to examine how sensitive the analysis is to the choice
of priors.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">type</span>
  <span class="hljs-type">Normal</span> = <span class="hljs-keyword">object</span>
    mu: <span class="hljs-built_in">float</span>
    sigma: <span class="hljs-built_in">float</span>

  <span class="hljs-type">Gamma</span> = <span class="hljs-keyword">object</span>
    k: <span class="hljs-built_in">float</span>
    sigma: <span class="hljs-built_in">float</span>

  <span class="hljs-type">Priors</span> = <span class="hljs-keyword">object</span>
    b0: <span class="hljs-type">Normal</span>
    b1: <span class="hljs-type">Normal</span>
    sd: <span class="hljs-type">Gamma</span>

<span class="hljs-keyword">var</span> priors = <span class="hljs-type">Priors</span>(
  b0: <span class="hljs-type">Normal</span>(mu:<span class="hljs-number">0.0</span>, sigma:<span class="hljs-number">10.0</span>),
  b1: <span class="hljs-type">Normal</span>(mu:<span class="hljs-number">1.0</span>, sigma:<span class="hljs-number">10.0</span>),
  sd: <span class="hljs-type">Gamma</span>(k:<span class="hljs-number">1.0</span>, sigma:<span class="hljs-number">10.0</span>))</code></pre>
<p>Now that we have prior probability density functions for each model parameter
that we are estimating, we need to be able to compute the joint prior probability
for all of the parameters of the model.
We will actually be using the $ln$ of the probabilities to
reduce rounding error since these values can be quite small.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> logPrior(priors: <span class="hljs-type">Priors</span>, b0, b1, sd: <span class="hljs-built_in">float</span> ): <span class="hljs-built_in">float</span> =
  <span class="hljs-keyword">let</span>
    b0Prior = ln(normalPdf(b0, priors.b0.mu, priors.b0.sigma))
    b1Prior = ln(normalPdf(b1, priors.b1.mu, priors.b1.sigma))
    sdPrior = ln(gammaPdf(sd, priors.sd.k, priors.sd.sigma))
  <span class="hljs-literal">result</span> = b0Prior + b1Prior + sdPrior</code></pre>
<h1>Likelihood</h1>
<p>We need to be able to calculate the likelihood of the observed $y_{i}$ values
given the observed $x_{i}$ values and the model parameter values, $\beta_{0}$,
$\beta_{1}$, and $\tau$.</p>
<p>We can write the model in a slightly different way:</p>
<p>$$\mu = \beta_{0} +\beta_{1} x$$
$$ y \sim N(\mu, \tau) $$</p>
<p>Then to compute the likelihood for a given set of $\beta_{0}$, $\beta_{1}$, $\tau$
parameters and data values $x_{i}$ and $y_{i}$ we use the normal probability
density function which we wrote before to compute our prior probabilities.
We will work with the $ln$ of the likelihood as we did with the priors.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> logLikelihood(x, y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], b0, b1, sd: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
  <span class="hljs-keyword">var</span> likelihoods = newSeq[<span class="hljs-built_in">float</span>](y.len)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; y.len:
    <span class="hljs-keyword">let</span> pred = b0 + (b1 * x[i])
    likelihoods[i] = ln(normalPdf(y[i], pred, sd))
  <span class="hljs-literal">result</span> = sum(likelihoods)</code></pre>
<h1>Posterior</h1>
<p>We cannot analytically solve the posterior probability distribution of our
linear model as the integration of the marginal likelihood is intractable.</p>
<p>But we can approximate it with markov chain monte carlo (MCMC) thanks to this
property of Bayes rule:</p>
<p>$$ \displaystyle P(\beta_{0}, \beta_{1}, \tau | Data) \propto
P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau) $$</p>
<p>The marginal likelihood is a normalizing constant. Without it, we no longer
have a probability density function. But the relative probability of a given set
of parameter values to another set is the same. We only care about which parameter
values are most probable so this is enough for us. We can determine which values
have higher probability by randomly walking through parameter space while accepting
or rejecting new values based on how probable they are.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> logPosterior(x, y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], priors: <span class="hljs-type">Priors</span>, b0, b1, sd: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
  <span class="hljs-keyword">let</span>
    like = logLikelihood(x, y, b0, b1, sd)
    prior = logPrior(priors, b0, b1, sd)
  <span class="hljs-literal">result</span> = like + prior</code></pre>
<h1>MCMC</h1>
<p>We will use a Metropolis-Hastings algorithm to approximate the unnormalized posterior.
The steps of this algorithm are as follows:</p>
<ol>
<li>
<p>Initialize arbitrary starting values for each model parameter.</p>
</li>
<li>
<p>Compute the unnormalized posterior probability density for the initialized
model parameter values given the observed data.</p>
</li>
<li>
<p>Propose a new value for one of the model parameters by randomly drawing from
a symetrical distribition centered on the present value. We will use a normal</p>
</li>
</ol>
<p>distribution.
4. Compute the unnormalized posterior probabity density for the proposed model
parameters values given the observed data.
5. Compute the ratio of the proposed probability density to the previous
probability density. $ \alpha = \frac{P(\beta_{0}\prime, \beta_{1}\prime, \tau\prime | Data)}{P(\beta_{0}, \beta_{1}, \tau | Data)} $
This is called the acceptance ratio.
6. All proposals with greater probability than the current state are accepted.
So a proposeal is accepted if the acceptance ratio is greater than 1. If the
acceptance ratio is less than 1 then it is accepted with probability \alpha. In practice we can make a
random draw from a uniform distribution ranging from 0 to 1 and accept proposals
anytime the acceptance ratio is less than the random uniform variate, $ \alpha &lt; r \sim Uniform(0, 1)$.
7. If a proposal is accepted then we will update the parameter in our model with
a new state. Otherwise the state will remain the same as before.
8. We then repeat steps 3-7 until reaching a desired number of iterations that
we believe give us an adequate sample from the unnormalized posterior distribution.</p>
<p>Note: When proposing values for $\tau$ from a normal distribution. It is possible for
proposals to be less than one. However, the gamma prior probability distribution
ranges from 0 to infinity meaning proposals of less than one are not valid and must
be rejected. Fortunately, this approach still gives us a proper sample of the targeted unnormalized posterior
probability distribution.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">type</span>
  <span class="hljs-type">MCMC</span> = <span class="hljs-keyword">object</span>
    x: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]
    y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]
    nSamples: <span class="hljs-built_in">int</span>
    priors: <span class="hljs-type">Priors</span>
    propSd: <span class="hljs-built_in">float</span>

  <span class="hljs-type">Samples</span> = <span class="hljs-keyword">object</span>
    n: <span class="hljs-built_in">int</span>
    b0: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]
    b1: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]
    sd: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]

<span class="hljs-keyword">proc</span> run(m: <span class="hljs-type">MCMC</span>, b0Start, b1Start, sdStart: <span class="hljs-built_in">float</span>): <span class="hljs-type">Samples</span> =
  <span class="hljs-keyword">var</span>
    b0Samples = newSeq[<span class="hljs-built_in">float</span>](m.nSamples)
    b1Samples = newSeq[<span class="hljs-built_in">float</span>](m.nSamples)
    sdSamples = newSeq[<span class="hljs-built_in">float</span>](m.nSamples)
    logProbs = newSeq[<span class="hljs-built_in">float</span>](m.nSamples)
  b0Samples[<span class="hljs-number">0</span>] = b0Start
  b1Samples[<span class="hljs-number">0</span>] = b1Start
  sdSamples[<span class="hljs-number">0</span>] = sdStart
  logProbs[<span class="hljs-number">0</span>] = logPosterior(m.x, m.y, m.priors, b0Start, b1Start, sdStart)

  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1.</span>.&lt;m.nSamples:
    <span class="hljs-keyword">let</span>
      b0Proposed = gauss(b0Samples[i-<span class="hljs-number">1</span>], m.propSd)
      b1Proposed = gauss(b1Samples[i-<span class="hljs-number">1</span>], m.propSd)
      sdProposed = gauss(sdSamples[i-<span class="hljs-number">1</span>], m.propSd)
    <span class="hljs-keyword">if</span> sdProposed &gt; <span class="hljs-number">0.0</span>:
      <span class="hljs-keyword">var</span>
        logProbProposed = logPosterior(m.x, m.y, m.priors, b0Proposed, b1Proposed, sdProposed)
        ratio = exp(logProbProposed - logProbs[i-<span class="hljs-number">1</span>])
      <span class="hljs-keyword">if</span> rand(<span class="hljs-number">1.0</span>) &lt; ratio:
        b0Samples[i] = b0proposed
        b1Samples[i] = b1proposed
        sdSamples[i] = sdproposed
        logProbs[i] = logProbProposed
      <span class="hljs-keyword">else</span>:
        b0Samples[i] = b0Samples[i-<span class="hljs-number">1</span>]
        b1Samples[i] = b1Samples[i-<span class="hljs-number">1</span>]
        sdSamples[i] = sdSamples[i-<span class="hljs-number">1</span>]
        logProbs[i] = logProbs[i-<span class="hljs-number">1</span>]
    <span class="hljs-keyword">else</span>:
      b0Samples[i] = b0Samples[i-<span class="hljs-number">1</span>]
      b1Samples[i] = b1Samples[i-<span class="hljs-number">1</span>]
      sdSamples[i] = sdSamples[i-<span class="hljs-number">1</span>]
      logProbs[i] = logProbs[i-<span class="hljs-number">1</span>]
  <span class="hljs-literal">result</span> = <span class="hljs-type">Samples</span>(n:m.nSamples, b0:b0Samples, b1:b1Samples, sd:sdSamples)</code></pre>
<p>Let's use this code to generate 100,000 samples. We'll cheat a bit and use the parameters
that the data were simulated under as starting values to speed things up. A standard deviation of 0.1
seems to work pretty well for the proposal distribution of each parameter.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span>
  mcmc = <span class="hljs-type">MCMC</span>(x:x, y:y, nSamples:<span class="hljs-number">100000</span>, priors:priors, propSd:<span class="hljs-number">0.1</span>)
  samples = mcmc.run(<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>)</code></pre>
<p>In some analyses, the posterior probability distribution could be multimodal
which might make the MCMC chain sensitive to the starting value as it may get stuck
in a local optimum and not sample from the full posterior distribtion. It is therefore
a good idea to acquire multiple samples with different starting values to see if they
converge on the same parameter estimates. Here we will run our MCMC one more time
with some different starting values</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span> samples2 = mcmc.run(<span class="hljs-number">0.2</span>, <span class="hljs-number">1.01</span>, <span class="hljs-number">1.1</span>)</code></pre>
<h1>Trace plots</h1>
<p>We can get a sense for how well our mcmc performed and therefore gain some
sense for how good our estimates might be by looking at the trace plots. Trace plots
show the parameter values stored during each step in the mcmc chain. Either
the accepted proposal value or the previous value if the proposal was rejected. Trace
plots can be an unreliable indicator of mcmc performance so it is a good
idea to assess it with other strategies as well.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">import</span> std/strformat

<span class="hljs-keyword">proc</span> plotTraces(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-type">Samples</span>], prefix: <span class="hljs-built_in">string</span>) =
  <span class="hljs-keyword">var</span>
    sample = newSeq[<span class="hljs-built_in">seq</span>[<span class="hljs-built_in">int</span>]](samples.len)
    chain = newSeq[<span class="hljs-built_in">seq</span>[<span class="hljs-built_in">int</span>]](samples.len)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; samples.len:
    sample[i] = toSeq(<span class="hljs-number">1</span> .. samples[i].n)
    chain[i] = repeat(i+<span class="hljs-number">1</span>, samples[i].n )
  <span class="hljs-keyword">var</span>
    df = toDf({
      <span class="hljs-string">&quot;sample&quot;</span>: concat(sample),
      <span class="hljs-string">&quot;chain&quot;</span>: concat(chain),
      <span class="hljs-string">&quot;b0&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b0)),
      <span class="hljs-string">&quot;b1&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b1)),
      <span class="hljs-string">&quot;sd&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.sd))
    })
  ggplot(df, aes(x=<span class="hljs-string">&quot;sample&quot;</span>, y=<span class="hljs-string">&quot;b0&quot;</span>)) +
    geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">fmt&quot;{prefix}b0.png&quot;</span>)

  ggplot(df, aes(x=<span class="hljs-string">&quot;sample&quot;</span>, y=<span class="hljs-string">&quot;b1&quot;</span>)) +
    geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">fmt&quot;{prefix}b1.png&quot;</span>)

  ggplot(df, aes(x=<span class="hljs-string">&quot;sample&quot;</span>, y=<span class="hljs-string">&quot;sd&quot;</span>)) +
    geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
    ggsave(<span class="hljs-string">fmt&quot;{prefix}sd.png&quot;</span>)

plotTraces(@[samples, samples2], <span class="hljs-string">&quot;images/trace-&quot;</span>)</code></pre><pre class="nb-output"><samp>INFO: The integer column `sample` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;sample&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
INFO: The integer column `sample` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;sample&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
INFO: The integer column `sample` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;sample&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.</samp></pre>
<figure>
<img src="./images/trace-b0.png" alt="">
<figcaption></figcaption>
</figure>
<figure>
<img src="./images/trace-b1.png" alt="">
<figcaption></figcaption>
</figure>
<figure>
<img src="./images/trace-sd.png" alt="">
<figcaption></figcaption>
</figure>
<h1>Burnin</h1>
<p>Initially the mcmc chain may spend time exploring unlikely regions of
parameter space. We can get a better approximation of the posterior if we
exclude these early steps in the chain. These excluded samples are referred to
as the burnin. A burnin of 10% seems to be more than enough with our informative priors
and starting values.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> burn(samples: <span class="hljs-type">Samples</span>, p: <span class="hljs-built_in">float</span>): <span class="hljs-type">Samples</span> =
  <span class="hljs-keyword">var</span>
    burnIx = (samples.n.<span class="hljs-built_in">float</span> * p).ceil.<span class="hljs-built_in">int</span>
    n = samples.n - burnIx
    b0 = samples.b0[burnIx..^<span class="hljs-number">1</span>]
    b1 = samples.b1[burnIx..^<span class="hljs-number">1</span>]
    sd = samples.sd[burnIx..^<span class="hljs-number">1</span>]
  <span class="hljs-literal">result</span> = <span class="hljs-type">Samples</span>(n:n, b0:b0, b1:b1, sd:sd)

<span class="hljs-keyword">var</span>
  burnin1 = burn(samples, <span class="hljs-number">0.1</span>)
  burnin2 = burn(samples2, <span class="hljs-number">0.1</span>)</code></pre>
<h1>Histograms</h1>
<p>Now that we have our post burnin samples, let's see what our posterior probability
distributions look like for each model paramter.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> plotHistograms(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-type">Samples</span>], prefix: <span class="hljs-built_in">string</span>) =
  <span class="hljs-keyword">var</span> chain = newSeq[<span class="hljs-built_in">seq</span>[<span class="hljs-built_in">int</span>]](samples.len)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; samples.len:
    chain[i] = repeat(i+<span class="hljs-number">1</span>, samples[i].n )
  <span class="hljs-keyword">var</span>
    df = toDf({
      <span class="hljs-string">&quot;chain&quot;</span>: concat(chain),
      <span class="hljs-string">&quot;b0&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b0)),
      <span class="hljs-string">&quot;b1&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b1)),
      <span class="hljs-string">&quot;sd&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.sd))
    })
  ggplot(df, aes(x=<span class="hljs-string">&quot;b0&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">fmt&quot;{prefix}b0.png&quot;</span>)

  ggplot(df, aes(x=<span class="hljs-string">&quot;b1&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">fmt&quot;{prefix}b1.png&quot;</span>)

  ggplot(df, aes(x=<span class="hljs-string">&quot;sd&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
    geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
    ggsave(<span class="hljs-string">fmt&quot;{prefix}sd.png&quot;</span>)

plotHistograms(@[burnin1, burnin2], <span class="hljs-string">&quot;images/hist-&quot;</span>)</code></pre><pre class="nb-output"><samp>INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.</samp></pre>
<figure>
<img src="./images/hist-b0.png" alt="">
<figcaption></figcaption>
</figure>
<figure>
<img src="./images/hist-b1.png" alt="">
<figcaption></figcaption>
</figure>
<figure>
<img src="./images/hist-sd.png" alt="">
<figcaption></figcaption>
</figure>
<h1>Posterior Summarization</h1>
<p>It's always great to visualize your results. But it's also useful to calculate some
summary statistics for the posterior. There are several different ways you could
do this.</p>
<h2>Combine Chains</h2>
<p>Before we summarize the posterior. Let's combine the samples into one.
Since we have determined that our MCMC is performing well, we can be confident
that each post burnin sample is a sample of the same distribution.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> concat(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-type">Samples</span>]): <span class="hljs-type">Samples</span> =
  <span class="hljs-keyword">var</span>
    n =  concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">int</span> = x.n))
    b0 = concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b0))
    b1 = concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b1))
    sd = concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.sd))
  <span class="hljs-literal">result</span> = <span class="hljs-type">Samples</span>(n:sum(n), b0:b0, b1:b1, sd:sd)

<span class="hljs-keyword">var</span> burnin = concat(@[burnin1, burnin2])</code></pre>
<h2>Posterior means</h2>
<p>One way to summarize the estimates of the posterior distributions is to simply calculate
the mean. Let's see how close these values are to the true values of the parameters.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">import</span> stats

<span class="hljs-keyword">var</span>
  meanB0 = mean(burnin.b0).round(<span class="hljs-number">3</span>)
  meanB1 = mean(burnin.b1).round(<span class="hljs-number">3</span>)
  meanSd = mean(burnin.sd).round(<span class="hljs-number">3</span>)

<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b0: &quot;</span>, meanB0
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b1: &quot;</span>, meanB1
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean sd: &quot;</span>, meanSd</code></pre><pre class="nb-output"><samp>Mean b0: 0.172
Mean b1: 0.997
Mean sd: 1.124</samp></pre>
<h2>Credible Intervals</h2>
<p>The means give us a point estimate for our parameter values but they tell us
nothing about the uncertainty of our estimates. We can get a sense for that by
looking at credible intervals. There are two widely used approaches for this,
equal tailed intervals, and highest density intervals. These will often match
each other closely when the target distribution is unimodal and symetric.
We will calculate the 89% interval for each of these below. Why 89%? Why not?
Credible interval threshold values are completely arbitrary.</p>
<h3>Equal Tailed Interval</h3>
<p>In this interval, the probability of being below the interval is equal to the
probability of being above the interval.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">import</span> algorithm

<span class="hljs-keyword">proc</span> quantile(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], interval: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
  <span class="hljs-keyword">let</span>
    s = sorted(samples, system.cmp[<span class="hljs-built_in">float</span>])
    k = <span class="hljs-built_in">float</span>(s.len - <span class="hljs-number">1</span>) * interval
    f = floor(k)
    c = ceil(k)
  <span class="hljs-keyword">if</span> f == c:
    <span class="hljs-literal">result</span> = s[<span class="hljs-built_in">int</span>(k)]
  <span class="hljs-keyword">else</span>:
    <span class="hljs-keyword">let</span>
      d0 = s[<span class="hljs-built_in">int</span>(f)] * (c - k)
      d1 = s[<span class="hljs-built_in">int</span>(c)] * (k - f)
    <span class="hljs-literal">result</span> = d0 + d1

<span class="hljs-keyword">proc</span> eti(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], interval: <span class="hljs-built_in">float</span>): (<span class="hljs-built_in">float</span>, <span class="hljs-built_in">float</span>) =
  <span class="hljs-keyword">let</span>
    p = (<span class="hljs-number">1</span> - interval) / <span class="hljs-number">2</span>
  <span class="hljs-keyword">let</span>
    q0 = quantile(samples, p)
    q1 = quantile(samples, <span class="hljs-number">1</span> - p)
  <span class="hljs-literal">result</span> = (q0, q1)

<span class="hljs-keyword">var</span>
  (b0EtiMin, b0EtiMax) = eti(burnin.b0, <span class="hljs-number">0.89</span>)
  (b1EtiMin, b1EtiMax) = eti(burnin.b1, <span class="hljs-number">0.89</span>)
  (sdEtiMin, sdEtiMax) = eti(burnin.sd, <span class="hljs-number">0.89</span>)

<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b0: &quot;</span>, b0EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b0EtiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b1: &quot;</span>, b1EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b1EtiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti sd: &quot;</span>, sdEtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, sdEtiMax.round(<span class="hljs-number">3</span>)</code></pre><pre class="nb-output"><samp>Eti b0: -0.178 - 0.527
Eti b1: 0.991 - 1.003
Eti sd: 1.005 - 1.26</samp></pre>
<h3>Highest Posterior Density Interval</h3>
<p>In this interval, all values inside of the interval have a higher probability
density than values outside of the interval.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> hdi(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], credMass: <span class="hljs-built_in">float</span>): (<span class="hljs-built_in">float</span>, <span class="hljs-built_in">float</span>) =
  <span class="hljs-keyword">let</span>
    sortedSamples = sorted(samples, system.cmp[<span class="hljs-built_in">float</span>])
    ciIdxInc = <span class="hljs-built_in">int</span>(floor(credMass * <span class="hljs-built_in">float</span>(sortedSamples.len)))
    nCIs = sortedSamples.len - ciIdxInc
  <span class="hljs-keyword">var</span> ciWidth = newSeq[<span class="hljs-built_in">float</span>](nCIs)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0.</span>.&lt;nCIs:
    ciWidth[i] = sortedSamples[i + ciIdxInc] - sortedSamples[i]
  <span class="hljs-keyword">let</span>
    minCiWidthIx = minIndex(ciWidth)
    hdiMin = sortedSamples[minCiWidthIx]
    hdiMax = sortedSamples[minCiWidthIx + ciIdxInc]
  <span class="hljs-literal">result</span> = (hdiMin, hdiMax)

<span class="hljs-keyword">var</span>
  (b0HdiMin, b0HdiMax) = hdi(burnin.b0, <span class="hljs-number">0.89</span>)
  (b1HdiMin, b1HdiMax) = hdi(burnin.b1, <span class="hljs-number">0.89</span>)
  (sdHdiMin, sdHdiMax) = hdi(burnin.sd, <span class="hljs-number">0.89</span>)

<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b0: &quot;</span>, b0HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b0HdiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b1: &quot;</span>, b1HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b1HdiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi sd: &quot;</span>, sdHdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, sdHdiMax.round(<span class="hljs-number">3</span>)</code></pre><pre class="nb-output"><samp>Hdi b0: -0.172 - 0.533
Hdi b1: 0.991 - 1.003
Hdi sd: 1.001 - 1.25</samp></pre>
<h1>Standardize Data</h1>
<p>The $\beta_{0}$ (intercept) and $\beta_{1}$ (slope) parameters of a linear model
present a bit of a challenge for MCMC because believable values for them are
tightly correlated. This means that a lot proposed values will be rejected and the
chain will not move efficiently. An easy way to get around this problem is
to standardize our data by rescaling them relative to their mean and standard deviation.</p>
<p>$$ \zeta_{x_{i}} = \frac{x_{i} - \bar{x}}{SD_{x}} $$
$$ \zeta_{y_{i}} = \frac{y_{i} - \bar{y}}{SD_{y}} $$</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span>
  xSt = newSeq[<span class="hljs-built_in">float</span>](n)
  ySt = newSeq[<span class="hljs-built_in">float</span>](n)
  xMean = mean(x)
  xSd = standardDeviation(x)
  yMean = mean(y)
  ySd = standardDeviation(y)</code></pre>
<p>We can see that both the $y$ and $x$ values are now centered around zero and have the same scale.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span> standardized = toDf(xSt, ySt)
ggplot(standardized, aes(x=<span class="hljs-string">&quot;xSt&quot;</span>, y=<span class="hljs-string">&quot;ySt&quot;</span>)) +
  geom_point() +
  ggsave(<span class="hljs-string">&quot;images/st-simulated-data.png&quot;</span>)</code></pre>
<figure>
<img src="./images/st-simulated-data.png" alt="">
<figcaption></figcaption>
</figure>
<p>We can now run the MCMC just as before with some minor changes. The standard deviation
$\tau$ for the standardized data is going to be much smaller. Let's make the prior
more informative. The standard deviation of the proposal distribution used before
would result in most proposals being rejected so we should use a smaller value
this time. Finally we should use some different starting values than before.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span>
  st_priors = <span class="hljs-type">Priors</span>(
    b0: <span class="hljs-type">Normal</span>(mu:<span class="hljs-number">0.0</span>, sigma:<span class="hljs-number">1.0</span>),
    b1: <span class="hljs-type">Normal</span>(mu:<span class="hljs-number">1.0</span>, sigma:<span class="hljs-number">1.0</span>),
    sd: <span class="hljs-type">Gamma</span>(k:<span class="hljs-number">0.0035</span>, sigma:<span class="hljs-number">1.0</span>))
  st_mcmc = <span class="hljs-type">MCMC</span>(x:xSt, y:ySt, nSamples:<span class="hljs-number">100000</span>, priors:st_priors, propSd:<span class="hljs-number">0.001</span>)
  st_samples1 = st_mcmc.run(<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0034</span>)
  st_samples2 = st_mcmc.run(<span class="hljs-number">0.1</span>, <span class="hljs-number">1.01</span>, <span class="hljs-number">0.0036</span>)</code></pre>
<h3>Convert back to original scale</h3>
<p>To interpret these estimates we need to convert back to the original scale.
$$ \beta_{0} = \zeta_{0} SD_{y} + \bar{y} - \zeta_{1} SD_{y} \bar{x} / SD_{x} $$
$$ \beta_{1} = \zeta_{1} SD_{y} / SD_{x} $$
$$ \tau = \zeta_{\tau} * SD_{y} $$</p>
<pre><code class="nim hljs"><span class="hljs-keyword">proc</span> backTransform(samples: <span class="hljs-type">Samples</span>, xMean, xSd, yMean, ySd: <span class="hljs-built_in">float</span>): <span class="hljs-type">Samples</span> =
  <span class="hljs-keyword">var</span>
    b0 = newSeq[<span class="hljs-built_in">float</span>](samples.n)
    b1 = newSeq[<span class="hljs-built_in">float</span>](samples.n)
    sd = newSeq[<span class="hljs-built_in">float</span>](samples.n)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; samples.n:
    b0[i] = samples.b0[i] * ySd + yMean - samples.b1[i] * ySd * xMean / xSd
    b1[i] = samples.b1[i] * ySd / xSd
    sd[i] = samples.sd[i] * ySd
  <span class="hljs-literal">result</span> = <span class="hljs-type">Samples</span>(n:samples.n, b0:b0, b1:b1, sd:sd)

<span class="hljs-keyword">var</span>
  st_samples_trans1 = backTransform(st_samples1, xMean, xSd, yMean, ySd)
  st_samples_trans2 = backTransform(st_samples2, xMean, xSd, yMean, ySd)</code></pre>
<h1>Traceplots</h1>
<p>Let's have a look at the trace plots.</p>
<pre><code class="nim hljs">plotTraces(@[st_samples_trans1, st_samples_trans2], <span class="hljs-string">&quot;images/trace-st-&quot;</span>)</code></pre><pre class="nb-output"><samp>INFO: The integer column `sample` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;sample&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
INFO: The integer column `sample` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;sample&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
INFO: The integer column `sample` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor(&quot;sample&quot;), ...)`.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.</samp></pre>
<figure>
<img src="./images/trace-st-b0.png" alt="">
<figcaption></figcaption>
</figure>
<figure>
<img src="./images/trace-st-b1.png" alt="">
<figcaption></figcaption>
</figure>
<figure>
<img src="./images/trace-st-sd.png" alt="">
<figcaption></figcaption>
</figure>
<p>It looks like more of the proposals were accepted and we have a much better
sample from the posterior.</p>
<h1>Burnin</h1>
<p>Let's get a post burnin sample as we did before.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span>
  st_burnin1 = burn(st_samples_trans1, <span class="hljs-number">0.1</span>)
  st_burnin2 = burn(st_samples_trans2, <span class="hljs-number">0.1</span>)</code></pre>
<h1>Histograms</h1>
<p>Now we can have a look at the posterior distribution of our estimates.</p>
<pre><code class="nim hljs">plotHistograms(@[st_burnin1, st_burnin2], <span class="hljs-string">&quot;images/hist-st-&quot;</span>)</code></pre><pre class="nb-output"><samp>INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.
INFO: The integer column `chain` has been automatically determined to be discrete. To overwrite this behavior add a `+ scale_x/y_continuous()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to.</samp></pre>
<figure>
<img src="./images/hist-st-b0.png" alt="">
<figcaption></figcaption>
</figure>
<figure>
<img src="./images/hist-st-b1.png" alt="">
<figcaption></figcaption>
</figure>
<figure>
<img src="./images/hist-st-sd.png" alt="">
<figcaption></figcaption>
</figure>
<p>These look pretty good but it's hard to know how they compare to the previous
MCMC estimates. Let's summarize the posterior to get a better idea.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">var</span> st_burnin = concat(@[st_burnin1, st_burnin2])

<span class="hljs-keyword">var</span>
  st_meanB0 = mean(st_burnin.b0).round(<span class="hljs-number">3</span>)
  st_meanB1 = mean(st_burnin.b1).round(<span class="hljs-number">3</span>)
  st_meanSd = mean(st_burnin.sd).round(<span class="hljs-number">3</span>)

<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b0: &quot;</span>, meanB0
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b1: &quot;</span>, meanB1
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean sd: &quot;</span>, meanSd

<span class="hljs-keyword">var</span>
  (st_b0EtiMin, st_b0EtiMax) = eti(st_burnin.b0, <span class="hljs-number">0.89</span>)
  (st_b1EtiMin, st_b1EtiMax) = eti(st_burnin.b1, <span class="hljs-number">0.89</span>)
  (st_sdEtiMin, st_sdEtiMax) = eti(st_burnin.sd, <span class="hljs-number">0.89</span>)

<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b0: &quot;</span>, st_b0EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_b0EtiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b1: &quot;</span>, st_b1EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_b1EtiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti sd: &quot;</span>, st_sdEtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_sdEtiMax.round(<span class="hljs-number">3</span>)

<span class="hljs-keyword">var</span>
  (st_b0HdiMin, st_b0HdiMax) = hdi(st_burnin.b0, <span class="hljs-number">0.89</span>)
  (st_b1HdiMin, st_b1HdiMax) = hdi(st_burnin.b1, <span class="hljs-number">0.89</span>)
  (st_sdHdiMin, st_sdHdiMax) = hdi(st_burnin.sd, <span class="hljs-number">0.89</span>)

<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b0: &quot;</span>, st_b0HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_b0HdiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b1: &quot;</span>, st_b1HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_b1HdiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi sd: &quot;</span>, st_sdHdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_sdHdiMax.round(<span class="hljs-number">3</span>)</code></pre><pre class="nb-output"><samp>Mean b0: 0.172
Mean b1: 0.997
Mean sd: 1.124
Eti b0: -0.167 - 0.516
Eti b1: 0.991 - 1.003
Eti sd: 0.993 - 1.25
Hdi b0: -0.17 - 0.513
Hdi b1: 0.991 - 1.003
Hdi sd: 0.985 - 1.24</samp></pre>
<p>In comparison to the previous mcmc results shown below, it looks like the standardized
analysis was actually pretty similar. But we can have greater confidence after
using the standardized analysis because we got a better posterior sample.</p>
<pre><code class="nim hljs"><span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b0: &quot;</span>, meanB0
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b1: &quot;</span>, meanB1
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean sd: &quot;</span>, meanSd

<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b0: &quot;</span>, b0EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b0EtiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b1: &quot;</span>, b1EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b1EtiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti sd: &quot;</span>, sdEtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, sdEtiMax.round(<span class="hljs-number">3</span>)

<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b0: &quot;</span>, b0HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b0HdiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b1: &quot;</span>, b1HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b1HdiMax.round(<span class="hljs-number">3</span>)
<span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi sd: &quot;</span>, sdHdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, sdHdiMax.round(<span class="hljs-number">3</span>)</code></pre><pre class="nb-output"><samp>Mean b0: 0.172
Mean b1: 0.997
Mean sd: 1.124
Eti b0: -0.178 - 0.527
Eti b1: 0.991 - 1.003
Eti sd: 1.005 - 1.26
Hdi b0: -0.172 - 0.533
Hdi b1: 0.991 - 1.003
Hdi sd: 1.001 - 1.25</samp></pre>
<h1>Final Note</h1>
<p>We could have simply and more efficiently done all of this using least squares regression.
However the Bayesian approach allows us to very easily and intuitively express
uncertainty about our estimates and can be easily extended to much more complex
models for which there are not such simple solutions. We could also incorporate prior
knowledge or assumptions in a way not possible with frequentist approaches.
The exercise above provides a starting point for this and shows just how easy it is to do with Nim!</p>
</main>
<footer>
<hr>
<div class="nb-box">
  <span><span class="nb-small">made with <a href="https://pietroppeter.github.io/nimib/">nimib üê≥</a></span></span>
  <span></span>
  <span><button class="nb-small" id="show" onclick="toggleSourceDisplay()">Show Source</button></span>
</div>
</footer>
<section id="source">
<pre><code class="nim hljs"><span class="hljs-keyword">import</span> nimib

nbInit
nbDoc.useLatex

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Bayesian Inference with Linear Model

At the time of this writing, Nim does not have any libraries for
conducting Bayesian inference. However, it is quite easy for us to
write all of the necessary code ourselves. This is a good exercise for learning
about Bayesian inference and the syntax and speed of Nim make it an excellent
choice for the this. In this tutorial we will walk through
the different parts needed to perform Bayesian inference with a linear model.
We will assume that you have some basic understanding of Bayesian
inference already. There are many excellent introductions available in books
and online.

Bayes theorem states:

$$ P(\theta|\text{Data}) = \frac{P(\text{Data}|\theta)P(\theta)}{P(\text{Data})}$$

Each of these terms are referred to as:

$$ \text{Posterior} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Marginal Likelihood}}$$

A marginal probability $P(\text{Data}) $ can be discrete probability distribution 
where $P(\text{Data}) = \sum_{\theta}P(\text{Data}|\theta)P(\theta)$ so the posterior
probability distribution is:

$$ P(\theta|\text{Data}) = \frac{P(\text{Data}|\theta)P(\theta)}{\sum_{\theta}P(\text{Data}|\theta)P(\theta)}$$

Or a marginal probability can be a continuous probability distribution
where $P(\text{Data}) = \int d\theta P(\text{Data}|\theta)P(\theta)$ so the posterior
probability distribution is:

$$ P(\theta|\text{Data}) = \frac{P(\text{Data}|\theta)P(\theta)}{\int d\theta P(\text{Data}|\theta)P(\theta)}$$

In this tutorial we will condsider a simple linear model
$ y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i} $ where $\epsilon \sim N(0, \tau)$.
Under this model, the parameters $\beta_0$ (y intercept) and $\beta_1$ (slope),
describe the relationship between a
predictor variable $x$ and a response variable $y$, with some unaccounted for
residual random error ($\epsilon$) which is normally distributed with a mean of $0$ and
standard deviation $\tau$.

We will estimate the values of the slope ($\beta_{0}$), the y-intercept
($\beta_{1}$), and the standard deviation of the residual random error ($\tau$)
from observed $x$ and $y$ data.

Expressing this with Bayes rule gives us:

$$ \displaystyle P(\beta_{0}, \beta_{1}, \tau | Data) =
  \frac{P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau)}
  {\iiint d\beta_{0} d\beta_{1} d\tau P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau)} $$


# Generate Data
We need some data to work with. Let's simulate data
under the model wth $\beta_{0}=0$, $\beta_{1}=1$, and $\tau=1$:

$ y = 0 + 1x + \epsilon$ where $\epsilon \sim N(0, 1) $

&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">import</span> std/sequtils, std/random, std/stats
  <span class="hljs-keyword">var</span>
    n = <span class="hljs-number">100</span>
    b0 = <span class="hljs-number">0.0</span>
    b1 = <span class="hljs-number">1.0</span>
    sd = <span class="hljs-number">1.0</span>
    x = newSeq[<span class="hljs-built_in">float</span>](n)
    y = newSeq[<span class="hljs-built_in">float</span>](n)
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; n:
    x[i] = rand(<span class="hljs-number">0.0</span>..<span class="hljs-number">100.0</span>)
    y[i] = b0 + (b1 * x[i]) + gauss(<span class="hljs-number">0.0</span>, sd)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
We can use `ggplotnim` to see what these data look like.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">import</span> datamancer, ggplotnim
  <span class="hljs-keyword">var</span> sim = toDf(x, y)
  ggplot(sim, aes(<span class="hljs-string">&quot;x&quot;</span>, <span class="hljs-string">&quot;y&quot;</span>)) +
    geom_point() +
    ggsave(<span class="hljs-string">&quot;images/simulated-data.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/simulated-data.png&quot;</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Priors
We need to choose prior probability distributions for each of the model parameters
that we are estimating.  Let's use a normal distribution for the priors on
$\beta_{0}$ and $\beta_{1}$. The $\tau$ parameter must be a positive value
greater than 0 so let's use the gamma distribution as the prior on $\tau$.

$$ \beta_{0} \sim Normal(\mu_{0}, \sigma^{2})$$
$$ \beta_{1} \sim Normal(\mu_{1}, \sigma^{2})$$
$$ \tau \sim Gamma(\kappa_{0}, \theta_{0})$$

To calculate the prior probabilities of proposed model parameter values, we will need
the proability density functions for these distributions.

#### Normal PDF
$$ p(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^{2}} $$

#### Gamma PDF
$$ p(x) = \frac{1}{\Gamma(k)\theta^{k}} x^{k-1} e^{-\frac{x}{\theta}} $$
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">import</span> std/math

  <span class="hljs-keyword">proc</span> normalPdf(x, m, s: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
    <span class="hljs-literal">result</span> = <span class="hljs-type">E</span>.pow((-<span class="hljs-number">0.5</span> * ((x - m) / s)^<span class="hljs-number">2</span>)) / (s * sqrt(<span class="hljs-number">2.0</span> * <span class="hljs-type">PI</span>))

  <span class="hljs-keyword">proc</span> gammaPdf(x, k, t: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
    <span class="hljs-literal">result</span> = x.pow(k - <span class="hljs-number">1.0</span>) * <span class="hljs-type">E</span>.pow(-(x / t)) / (gamma(k) * t.pow(k))

nbText: <span class="hljs-string">md&quot;&quot;&quot;
We now need to decide how to parameterize these prior probability densities.
Since this is for the purpose of demonstration, let's use very informed
priors so that we can quickly get a good sample from the posterior.

$$ \beta_{0} \sim Normal(0, 1)$$
$$ \beta_{1} \sim Normal(1, 1)$$
$$ \tau \sim Gamma(1, 1)$$

In a real analysis, the priors should encompass all values that we consider possible and
it may also be a good idea to examine how sensitive the analysis is to the choice
of priors.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">type</span>
    <span class="hljs-type">Normal</span> = <span class="hljs-keyword">object</span>
      mu: <span class="hljs-built_in">float</span>
      sigma: <span class="hljs-built_in">float</span>

    <span class="hljs-type">Gamma</span> = <span class="hljs-keyword">object</span>
      k: <span class="hljs-built_in">float</span>
      sigma: <span class="hljs-built_in">float</span>

    <span class="hljs-type">Priors</span> = <span class="hljs-keyword">object</span>
      b0: <span class="hljs-type">Normal</span>
      b1: <span class="hljs-type">Normal</span>
      sd: <span class="hljs-type">Gamma</span>

  <span class="hljs-keyword">var</span> priors = <span class="hljs-type">Priors</span>(
    b0: <span class="hljs-type">Normal</span>(mu:<span class="hljs-number">0.0</span>, sigma:<span class="hljs-number">10.0</span>),
    b1: <span class="hljs-type">Normal</span>(mu:<span class="hljs-number">1.0</span>, sigma:<span class="hljs-number">10.0</span>),
    sd: <span class="hljs-type">Gamma</span>(k:<span class="hljs-number">1.0</span>, sigma:<span class="hljs-number">10.0</span>))

nbText: <span class="hljs-string">md&quot;&quot;&quot;
Now that we have prior probability density functions for each model parameter
that we are estimating, we need to be able to compute the joint prior probability
for all of the parameters of the model.
We will actually be using the $ln$ of the probabilities to
reduce rounding error since these values can be quite small.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">proc</span> logPrior(priors: <span class="hljs-type">Priors</span>, b0, b1, sd: <span class="hljs-built_in">float</span> ): <span class="hljs-built_in">float</span> =
    <span class="hljs-keyword">let</span>
      b0Prior = ln(normalPdf(b0, priors.b0.mu, priors.b0.sigma))
      b1Prior = ln(normalPdf(b1, priors.b1.mu, priors.b1.sigma))
      sdPrior = ln(gammaPdf(sd, priors.sd.k, priors.sd.sigma))
    <span class="hljs-literal">result</span> = b0Prior + b1Prior + sdPrior

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Likelihood
We need to be able to calculate the likelihood of the observed $y_{i}$ values
given the observed $x_{i}$ values and the model parameter values, $\beta_{0}$,
$\beta_{1}$, and $\tau$.

We can write the model in a slightly different way:

$$\mu = \beta_{0} +\beta_{1} x$$
$$ y \sim N(\mu, \tau) $$

Then to compute the likelihood for a given set of $\beta_{0}$, $\beta_{1}$, $\tau$
parameters and data values $x_{i}$ and $y_{i}$ we use the normal probability
density function which we wrote before to compute our prior probabilities.
We will work with the $ln$ of the likelihood as we did with the priors.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">proc</span> logLikelihood(x, y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], b0, b1, sd: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
    <span class="hljs-keyword">var</span> likelihoods = newSeq[<span class="hljs-built_in">float</span>](y.len)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; y.len:
      <span class="hljs-keyword">let</span> pred = b0 + (b1 * x[i])
      likelihoods[i] = ln(normalPdf(y[i], pred, sd))
    <span class="hljs-literal">result</span> = sum(likelihoods)

<span class="hljs-comment">#TODO: Finish</span>
nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Posterior
We cannot analytically solve the posterior probability distribution of our
linear model as the integration of the marginal likelihood is intractable.

But we can approximate it with markov chain monte carlo (MCMC) thanks to this
property of Bayes rule:

$$ \displaystyle P(\beta_{0}, \beta_{1}, \tau | Data) \propto
  P(Data|\beta_{0}, \beta_{1}, \tau) P(\beta_{0}, \beta_{1}, \tau) $$

The marginal likelihood is a normalizing constant. Without it, we no longer
have a probability density function. But the relative probability of a given set
of parameter values to another set is the same. We only care about which parameter
values are most probable so this is enough for us. We can determine which values
have higher probability by randomly walking through parameter space while accepting
or rejecting new values based on how probable they are.

&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">proc</span> logPosterior(x, y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], priors: <span class="hljs-type">Priors</span>, b0, b1, sd: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
    <span class="hljs-keyword">let</span>
      like = logLikelihood(x, y, b0, b1, sd)
      prior = logPrior(priors, b0, b1, sd)
    <span class="hljs-literal">result</span> = like + prior

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# MCMC
We will use a Metropolis-Hastings algorithm to approximate the unnormalized posterior.
The steps of this algorithm are as follows:
1. Initialize arbitrary starting values for each model parameter.
2. Compute the unnormalized posterior probability density for the initialized
model parameter values given the observed data.
3. Propose a new value for one of the model parameters by randomly drawing from
a symetrical distribition centered on the present value. We will use a normal
distribution. 
4. Compute the unnormalized posterior probabity density for the proposed model
parameters values given the observed data. 
5. Compute the ratio of the proposed probability density to the previous
probability density. $ \alpha = \frac{P(\beta_{0}\prime, \beta_{1}\prime, \tau\prime | Data)}{P(\beta_{0}, \beta_{1}, \tau | Data)} $
This is called the acceptance ratio. 
6. All proposals with greater probability than the current state are accepted.
So a proposeal is accepted if the acceptance ratio is greater than 1. If the
acceptance ratio is less than 1 then it is accepted with probability \alpha. In practice we can make a
random draw from a uniform distribution ranging from 0 to 1 and accept proposals
anytime the acceptance ratio is less than the random uniform variate, $ \alpha &lt; r \sim Uniform(0, 1)$. 
7. If a proposal is accepted then we will update the parameter in our model with
a new state. Otherwise the state will remain the same as before. 
8. We then repeat steps 3-7 until reaching a desired number of iterations that
we believe give us an adequate sample from the unnormalized posterior distribution. 

Note: When proposing values for $\tau$ from a normal distribution. It is possible for
proposals to be less than one. However, the gamma prior probability distribution
ranges from 0 to infinity meaning proposals of less than one are not valid and must
be rejected. Fortunately, this approach still gives us a proper sample of the targeted unnormalized posterior
probability distribution.

&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">type</span>
    <span class="hljs-type">MCMC</span> = <span class="hljs-keyword">object</span>
      x: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]
      y: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]
      nSamples: <span class="hljs-built_in">int</span>
      priors: <span class="hljs-type">Priors</span>
      propSd: <span class="hljs-built_in">float</span>

    <span class="hljs-type">Samples</span> = <span class="hljs-keyword">object</span>
      n: <span class="hljs-built_in">int</span>
      b0: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]
      b1: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]
      sd: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>]

  <span class="hljs-keyword">proc</span> run(m: <span class="hljs-type">MCMC</span>, b0Start, b1Start, sdStart: <span class="hljs-built_in">float</span>): <span class="hljs-type">Samples</span> =
    <span class="hljs-keyword">var</span>
      b0Samples = newSeq[<span class="hljs-built_in">float</span>](m.nSamples)
      b1Samples = newSeq[<span class="hljs-built_in">float</span>](m.nSamples)
      sdSamples = newSeq[<span class="hljs-built_in">float</span>](m.nSamples)
      logProbs = newSeq[<span class="hljs-built_in">float</span>](m.nSamples)
    b0Samples[<span class="hljs-number">0</span>] = b0Start
    b1Samples[<span class="hljs-number">0</span>] = b1Start
    sdSamples[<span class="hljs-number">0</span>] = sdStart
    logProbs[<span class="hljs-number">0</span>] = logPosterior(m.x, m.y, m.priors, b0Start, b1Start, sdStart)

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1.</span>.&lt;m.nSamples:
      <span class="hljs-keyword">let</span>
        b0Proposed = gauss(b0Samples[i-<span class="hljs-number">1</span>], m.propSd)
        b1Proposed = gauss(b1Samples[i-<span class="hljs-number">1</span>], m.propSd)
        sdProposed = gauss(sdSamples[i-<span class="hljs-number">1</span>], m.propSd)
      <span class="hljs-keyword">if</span> sdProposed &gt; <span class="hljs-number">0.0</span>:
        <span class="hljs-keyword">var</span>
          logProbProposed = logPosterior(m.x, m.y, m.priors, b0Proposed, b1Proposed, sdProposed)
          ratio = exp(logProbProposed - logProbs[i-<span class="hljs-number">1</span>])
        <span class="hljs-keyword">if</span> rand(<span class="hljs-number">1.0</span>) &lt; ratio:
          b0Samples[i] = b0proposed
          b1Samples[i] = b1proposed
          sdSamples[i] = sdproposed
          logProbs[i] = logProbProposed
        <span class="hljs-keyword">else</span>:
          b0Samples[i] = b0Samples[i-<span class="hljs-number">1</span>]
          b1Samples[i] = b1Samples[i-<span class="hljs-number">1</span>]
          sdSamples[i] = sdSamples[i-<span class="hljs-number">1</span>]
          logProbs[i] = logProbs[i-<span class="hljs-number">1</span>]
      <span class="hljs-keyword">else</span>:
        b0Samples[i] = b0Samples[i-<span class="hljs-number">1</span>]
        b1Samples[i] = b1Samples[i-<span class="hljs-number">1</span>]
        sdSamples[i] = sdSamples[i-<span class="hljs-number">1</span>]
        logProbs[i] = logProbs[i-<span class="hljs-number">1</span>]
    <span class="hljs-literal">result</span> = <span class="hljs-type">Samples</span>(n:m.nSamples, b0:b0Samples, b1:b1Samples, sd:sdSamples)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
Let's use this code to generate 100,000 samples. We'll cheat a bit and use the parameters
that the data were simulated under as starting values to speed things up. A standard deviation of 0.1
seems to work pretty well for the proposal distribution of each parameter.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">var</span>
    mcmc = <span class="hljs-type">MCMC</span>(x:x, y:y, nSamples:<span class="hljs-number">100000</span>, priors:priors, propSd:<span class="hljs-number">0.1</span>)
    samples = mcmc.run(<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
In some analyses, the posterior probability distribution could be multimodal
which might make the MCMC chain sensitive to the starting value as it may get stuck
in a local optimum and not sample from the full posterior distribtion. It is therefore
a good idea to acquire multiple samples with different starting values to see if they
converge on the same parameter estimates. Here we will run our MCMC one more time
with some different starting values
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">var</span> samples2 = mcmc.run(<span class="hljs-number">0.2</span>, <span class="hljs-number">1.01</span>, <span class="hljs-number">1.1</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Trace plots
We can get a sense for how well our mcmc performed and therefore gain some
sense for how good our estimates might be by looking at the trace plots. Trace plots
show the parameter values stored during each step in the mcmc chain. Either
the accepted proposal value or the previous value if the proposal was rejected. Trace
plots can be an unreliable indicator of mcmc performance so it is a good
idea to assess it with other strategies as well.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">import</span> std/strformat

  <span class="hljs-keyword">proc</span> plotTraces(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-type">Samples</span>], prefix: <span class="hljs-built_in">string</span>) =
    <span class="hljs-keyword">var</span>
      sample = newSeq[<span class="hljs-built_in">seq</span>[<span class="hljs-built_in">int</span>]](samples.len)
      chain = newSeq[<span class="hljs-built_in">seq</span>[<span class="hljs-built_in">int</span>]](samples.len)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; samples.len:
      sample[i] = toSeq(<span class="hljs-number">1</span> .. samples[i].n)
      chain[i] = repeat(i+<span class="hljs-number">1</span>, samples[i].n )
    <span class="hljs-keyword">var</span>
      df = toDf({
        <span class="hljs-string">&quot;sample&quot;</span>: concat(sample),
        <span class="hljs-string">&quot;chain&quot;</span>: concat(chain),
        <span class="hljs-string">&quot;b0&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b0)),
        <span class="hljs-string">&quot;b1&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b1)),
        <span class="hljs-string">&quot;sd&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.sd))
      })
    ggplot(df, aes(x=<span class="hljs-string">&quot;sample&quot;</span>, y=<span class="hljs-string">&quot;b0&quot;</span>)) +
      geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
      ggsave(<span class="hljs-string">fmt&quot;{prefix}b0.png&quot;</span>)

    ggplot(df, aes(x=<span class="hljs-string">&quot;sample&quot;</span>, y=<span class="hljs-string">&quot;b1&quot;</span>)) +
      geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
      ggsave(<span class="hljs-string">fmt&quot;{prefix}b1.png&quot;</span>)

    ggplot(df, aes(x=<span class="hljs-string">&quot;sample&quot;</span>, y=<span class="hljs-string">&quot;sd&quot;</span>)) +
      geom_line(aes(color=<span class="hljs-string">&quot;chain&quot;</span>)) +
      ggsave(<span class="hljs-string">fmt&quot;{prefix}sd.png&quot;</span>)

  plotTraces(@[samples, samples2], <span class="hljs-string">&quot;images/trace-&quot;</span>)

nbImage(<span class="hljs-string">&quot;images/trace-b0.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/trace-b1.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/trace-sd.png&quot;</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Burnin
Initially the mcmc chain may spend time exploring unlikely regions of
parameter space. We can get a better approximation of the posterior if we
exclude these early steps in the chain. These excluded samples are referred to
as the burnin. A burnin of 10% seems to be more than enough with our informative priors
and starting values.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">proc</span> burn(samples: <span class="hljs-type">Samples</span>, p: <span class="hljs-built_in">float</span>): <span class="hljs-type">Samples</span> =
    <span class="hljs-keyword">var</span>
      burnIx = (samples.n.<span class="hljs-built_in">float</span> * p).ceil.<span class="hljs-built_in">int</span>
      n = samples.n - burnIx
      b0 = samples.b0[burnIx..^<span class="hljs-number">1</span>]
      b1 = samples.b1[burnIx..^<span class="hljs-number">1</span>]
      sd = samples.sd[burnIx..^<span class="hljs-number">1</span>]
    <span class="hljs-literal">result</span> = <span class="hljs-type">Samples</span>(n:n, b0:b0, b1:b1, sd:sd)

  <span class="hljs-keyword">var</span>
    burnin1 = burn(samples, <span class="hljs-number">0.1</span>)
    burnin2 = burn(samples2, <span class="hljs-number">0.1</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Histograms
Now that we have our post burnin samples, let's see what our posterior probability
distributions look like for each model paramter.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">proc</span> plotHistograms(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-type">Samples</span>], prefix: <span class="hljs-built_in">string</span>) =
    <span class="hljs-keyword">var</span> chain = newSeq[<span class="hljs-built_in">seq</span>[<span class="hljs-built_in">int</span>]](samples.len)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; samples.len:
      chain[i] = repeat(i+<span class="hljs-number">1</span>, samples[i].n )
    <span class="hljs-keyword">var</span>
      df = toDf({
        <span class="hljs-string">&quot;chain&quot;</span>: concat(chain),
        <span class="hljs-string">&quot;b0&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b0)),
        <span class="hljs-string">&quot;b1&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b1)),
        <span class="hljs-string">&quot;sd&quot;</span>: concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.sd))
      })
    ggplot(df, aes(x=<span class="hljs-string">&quot;b0&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
      geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
      ggsave(<span class="hljs-string">fmt&quot;{prefix}b0.png&quot;</span>)

    ggplot(df, aes(x=<span class="hljs-string">&quot;b1&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
      geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
      ggsave(<span class="hljs-string">fmt&quot;{prefix}b1.png&quot;</span>)

    ggplot(df, aes(x=<span class="hljs-string">&quot;sd&quot;</span>, fill=<span class="hljs-string">&quot;chain&quot;</span>)) +
      geom_histogram(position=<span class="hljs-string">&quot;identity&quot;</span>, alpha=some(<span class="hljs-number">0.5</span>)) +
      ggsave(<span class="hljs-string">fmt&quot;{prefix}sd.png&quot;</span>)

  plotHistograms(@[burnin1, burnin2], <span class="hljs-string">&quot;images/hist-&quot;</span>)

nbImage(<span class="hljs-string">&quot;images/hist-b0.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/hist-b1.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/hist-sd.png&quot;</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Posterior Summarization
It's always great to visualize your results. But it's also useful to calculate some
summary statistics for the posterior. There are several different ways you could
do this.

## Combine Chains
Before we summarize the posterior. Let's combine the samples into one.
Since we have determined that our MCMC is performing well, we can be confident
that each post burnin sample is a sample of the same distribution.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">proc</span> concat(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-type">Samples</span>]): <span class="hljs-type">Samples</span> =
    <span class="hljs-keyword">var</span>
      n =  concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">int</span> = x.n))
      b0 = concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b0))
      b1 = concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.b1))
      sd = concat(map(samples, <span class="hljs-keyword">proc</span>(x: <span class="hljs-type">Samples</span>): <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>] = x.sd))
    <span class="hljs-literal">result</span> = <span class="hljs-type">Samples</span>(n:sum(n), b0:b0, b1:b1, sd:sd)

  <span class="hljs-keyword">var</span> burnin = concat(@[burnin1, burnin2])

nbText: <span class="hljs-string">md&quot;&quot;&quot;
## Posterior means
One way to summarize the estimates of the posterior distributions is to simply calculate
the mean. Let's see how close these values are to the true values of the parameters.
 &quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">import</span> stats

  <span class="hljs-keyword">var</span>
    meanB0 = mean(burnin.b0).round(<span class="hljs-number">3</span>)
    meanB1 = mean(burnin.b1).round(<span class="hljs-number">3</span>)
    meanSd = mean(burnin.sd).round(<span class="hljs-number">3</span>)

  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b0: &quot;</span>, meanB0
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b1: &quot;</span>, meanB1
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean sd: &quot;</span>, meanSd

nbText: <span class="hljs-string">md&quot;&quot;&quot;
## Credible Intervals
The means give us a point estimate for our parameter values but they tell us
nothing about the uncertainty of our estimates. We can get a sense for that by
looking at credible intervals. There are two widely used approaches for this,
equal tailed intervals, and highest density intervals. These will often match
each other closely when the target distribution is unimodal and symetric.
We will calculate the 89% interval for each of these below. Why 89%? Why not?
Credible interval threshold values are completely arbitrary.

### Equal Tailed Interval
In this interval, the probability of being below the interval is equal to the 
probability of being above the interval.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">import</span> algorithm

  <span class="hljs-keyword">proc</span> quantile(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], interval: <span class="hljs-built_in">float</span>): <span class="hljs-built_in">float</span> =
    <span class="hljs-keyword">let</span>
      s = sorted(samples, system.cmp[<span class="hljs-built_in">float</span>])
      k = <span class="hljs-built_in">float</span>(s.len - <span class="hljs-number">1</span>) * interval
      f = floor(k)
      c = ceil(k)
    <span class="hljs-keyword">if</span> f == c:
      <span class="hljs-literal">result</span> = s[<span class="hljs-built_in">int</span>(k)]
    <span class="hljs-keyword">else</span>:
      <span class="hljs-keyword">let</span>
        d0 = s[<span class="hljs-built_in">int</span>(f)] * (c - k)
        d1 = s[<span class="hljs-built_in">int</span>(c)] * (k - f)
      <span class="hljs-literal">result</span> = d0 + d1

  <span class="hljs-keyword">proc</span> eti(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], interval: <span class="hljs-built_in">float</span>): (<span class="hljs-built_in">float</span>, <span class="hljs-built_in">float</span>) =
    <span class="hljs-keyword">let</span>
      p = (<span class="hljs-number">1</span> - interval) / <span class="hljs-number">2</span>
    <span class="hljs-keyword">let</span>
      q0 = quantile(samples, p)
      q1 = quantile(samples, <span class="hljs-number">1</span> - p)
    <span class="hljs-literal">result</span> = (q0, q1)

  <span class="hljs-keyword">var</span>
    (b0EtiMin, b0EtiMax) = eti(burnin.b0, <span class="hljs-number">0.89</span>)
    (b1EtiMin, b1EtiMax) = eti(burnin.b1, <span class="hljs-number">0.89</span>)
    (sdEtiMin, sdEtiMax) = eti(burnin.sd, <span class="hljs-number">0.89</span>)

  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b0: &quot;</span>, b0EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b0EtiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b1: &quot;</span>, b1EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b1EtiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti sd: &quot;</span>, sdEtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, sdEtiMax.round(<span class="hljs-number">3</span>)


nbText: <span class="hljs-string">md&quot;&quot;&quot;
### Highest Posterior Density Interval
In this interval, all values inside of the interval have a higher probability 
density than values outside of the interval.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">proc</span> hdi(samples: <span class="hljs-built_in">seq</span>[<span class="hljs-built_in">float</span>], credMass: <span class="hljs-built_in">float</span>): (<span class="hljs-built_in">float</span>, <span class="hljs-built_in">float</span>) =
    <span class="hljs-keyword">let</span>
      sortedSamples = sorted(samples, system.cmp[<span class="hljs-built_in">float</span>])
      ciIdxInc = <span class="hljs-built_in">int</span>(floor(credMass * <span class="hljs-built_in">float</span>(sortedSamples.len)))
      nCIs = sortedSamples.len - ciIdxInc
    <span class="hljs-keyword">var</span> ciWidth = newSeq[<span class="hljs-built_in">float</span>](nCIs)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0.</span>.&lt;nCIs:
      ciWidth[i] = sortedSamples[i + ciIdxInc] - sortedSamples[i]
    <span class="hljs-keyword">let</span>
      minCiWidthIx = minIndex(ciWidth)
      hdiMin = sortedSamples[minCiWidthIx]
      hdiMax = sortedSamples[minCiWidthIx + ciIdxInc]
    <span class="hljs-literal">result</span> = (hdiMin, hdiMax)

  <span class="hljs-keyword">var</span>
    (b0HdiMin, b0HdiMax) = hdi(burnin.b0, <span class="hljs-number">0.89</span>)
    (b1HdiMin, b1HdiMax) = hdi(burnin.b1, <span class="hljs-number">0.89</span>)
    (sdHdiMin, sdHdiMax) = hdi(burnin.sd, <span class="hljs-number">0.89</span>)

  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b0: &quot;</span>, b0HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b0HdiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b1: &quot;</span>, b1HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b1HdiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi sd: &quot;</span>, sdHdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, sdHdiMax.round(<span class="hljs-number">3</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Standardize Data
The $\beta_{0}$ (intercept) and $\beta_{1}$ (slope) parameters of a linear model
present a bit of a challenge for MCMC because believable values for them are
tightly correlated. This means that a lot proposed values will be rejected and the
chain will not move efficiently. An easy way to get around this problem is
to standardize our data by rescaling them relative to their mean and standard deviation.

$$ \zeta_{x_{i}} = \frac{x_{i} - \bar{x}}{SD_{x}} $$
$$ \zeta_{y_{i}} = \frac{y_{i} - \bar{y}}{SD_{y}} $$
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">var</span>
    xSt = newSeq[<span class="hljs-built_in">float</span>](n)
    ySt = newSeq[<span class="hljs-built_in">float</span>](n)
    xMean = mean(x)
    xSd = standardDeviation(x)
    yMean = mean(y)
    ySd = standardDeviation(y)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; n:
  xSt[i] = (x[i] - xMean) / xSd
  ySt[i] = (y[i] - yMean) / ySd

nbText: <span class="hljs-string">md&quot;&quot;&quot;
We can see that both the $y$ and $x$ values are now centered around zero and have the same scale.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">var</span> standardized = toDf(xSt, ySt)
  ggplot(standardized, aes(x=<span class="hljs-string">&quot;xSt&quot;</span>, y=<span class="hljs-string">&quot;ySt&quot;</span>)) +
    geom_point() +
    ggsave(<span class="hljs-string">&quot;images/st-simulated-data.png&quot;</span>)

nbImage(<span class="hljs-string">&quot;images/st-simulated-data.png&quot;</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
We can now run the MCMC just as before with some minor changes. The standard deviation
$\tau$ for the standardized data is going to be much smaller. Let's make the prior
more informative. The standard deviation of the proposal distribution used before
would result in most proposals being rejected so we should use a smaller value
this time. Finally we should use some different starting values than before.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">var</span>
    st_priors = <span class="hljs-type">Priors</span>(
      b0: <span class="hljs-type">Normal</span>(mu:<span class="hljs-number">0.0</span>, sigma:<span class="hljs-number">1.0</span>),
      b1: <span class="hljs-type">Normal</span>(mu:<span class="hljs-number">1.0</span>, sigma:<span class="hljs-number">1.0</span>),
      sd: <span class="hljs-type">Gamma</span>(k:<span class="hljs-number">0.0035</span>, sigma:<span class="hljs-number">1.0</span>))
    st_mcmc = <span class="hljs-type">MCMC</span>(x:xSt, y:ySt, nSamples:<span class="hljs-number">100000</span>, priors:st_priors, propSd:<span class="hljs-number">0.001</span>)
    st_samples1 = st_mcmc.run(<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.0034</span>)
    st_samples2 = st_mcmc.run(<span class="hljs-number">0.1</span>, <span class="hljs-number">1.01</span>, <span class="hljs-number">0.0036</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
### Convert back to original scale
To interpret these estimates we need to convert back to the original scale.
$$ \beta_{0} = \zeta_{0} SD_{y} + \bar{y} - \zeta_{1} SD_{y} \bar{x} / SD_{x} $$
$$ \beta_{1} = \zeta_{1} SD_{y} / SD_{x} $$
$$ \tau = \zeta_{\tau} * SD_{y} $$
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">proc</span> backTransform(samples: <span class="hljs-type">Samples</span>, xMean, xSd, yMean, ySd: <span class="hljs-built_in">float</span>): <span class="hljs-type">Samples</span> =
    <span class="hljs-keyword">var</span>
      b0 = newSeq[<span class="hljs-built_in">float</span>](samples.n)
      b1 = newSeq[<span class="hljs-built_in">float</span>](samples.n)
      sd = newSeq[<span class="hljs-built_in">float</span>](samples.n)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> ..&lt; samples.n:
      b0[i] = samples.b0[i] * ySd + yMean - samples.b1[i] * ySd * xMean / xSd
      b1[i] = samples.b1[i] * ySd / xSd
      sd[i] = samples.sd[i] * ySd
    <span class="hljs-literal">result</span> = <span class="hljs-type">Samples</span>(n:samples.n, b0:b0, b1:b1, sd:sd)

  <span class="hljs-keyword">var</span>
    st_samples_trans1 = backTransform(st_samples1, xMean, xSd, yMean, ySd)
    st_samples_trans2 = backTransform(st_samples2, xMean, xSd, yMean, ySd)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Traceplots
Let's have a look at the trace plots.
&quot;&quot;&quot;</span>

nbCode:
  plotTraces(@[st_samples_trans1, st_samples_trans2], <span class="hljs-string">&quot;images/trace-st-&quot;</span>)

nbImage(<span class="hljs-string">&quot;images/trace-st-b0.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/trace-st-b1.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/trace-st-sd.png&quot;</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
It looks like more of the proposals were accepted and we have a much better
sample from the posterior.
&quot;&quot;&quot;</span>

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Burnin
Let's get a post burnin sample as we did before.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">var</span>
    st_burnin1 = burn(st_samples_trans1, <span class="hljs-number">0.1</span>)
    st_burnin2 = burn(st_samples_trans2, <span class="hljs-number">0.1</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Histograms
Now we can have a look at the posterior distribution of our estimates.
&quot;&quot;&quot;</span>

nbCode:
  plotHistograms(@[st_burnin1, st_burnin2], <span class="hljs-string">&quot;images/hist-st-&quot;</span>)


nbImage(<span class="hljs-string">&quot;images/hist-st-b0.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/hist-st-b1.png&quot;</span>)
nbImage(<span class="hljs-string">&quot;images/hist-st-sd.png&quot;</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
These look pretty good but it's hard to know how they compare to the previous
MCMC estimates. Let's summarize the posterior to get a better idea.
&quot;&quot;&quot;</span>

nbCode:

  <span class="hljs-keyword">var</span> st_burnin = concat(@[st_burnin1, st_burnin2])

  <span class="hljs-keyword">var</span>
    st_meanB0 = mean(st_burnin.b0).round(<span class="hljs-number">3</span>)
    st_meanB1 = mean(st_burnin.b1).round(<span class="hljs-number">3</span>)
    st_meanSd = mean(st_burnin.sd).round(<span class="hljs-number">3</span>)

  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b0: &quot;</span>, meanB0
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b1: &quot;</span>, meanB1
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean sd: &quot;</span>, meanSd

  <span class="hljs-keyword">var</span>
    (st_b0EtiMin, st_b0EtiMax) = eti(st_burnin.b0, <span class="hljs-number">0.89</span>)
    (st_b1EtiMin, st_b1EtiMax) = eti(st_burnin.b1, <span class="hljs-number">0.89</span>)
    (st_sdEtiMin, st_sdEtiMax) = eti(st_burnin.sd, <span class="hljs-number">0.89</span>)

  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b0: &quot;</span>, st_b0EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_b0EtiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b1: &quot;</span>, st_b1EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_b1EtiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti sd: &quot;</span>, st_sdEtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_sdEtiMax.round(<span class="hljs-number">3</span>)

  <span class="hljs-keyword">var</span>
    (st_b0HdiMin, st_b0HdiMax) = hdi(st_burnin.b0, <span class="hljs-number">0.89</span>)
    (st_b1HdiMin, st_b1HdiMax) = hdi(st_burnin.b1, <span class="hljs-number">0.89</span>)
    (st_sdHdiMin, st_sdHdiMax) = hdi(st_burnin.sd, <span class="hljs-number">0.89</span>)

  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b0: &quot;</span>, st_b0HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_b0HdiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b1: &quot;</span>, st_b1HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_b1HdiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi sd: &quot;</span>, st_sdHdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, st_sdHdiMax.round(<span class="hljs-number">3</span>)


nbText: <span class="hljs-string">md&quot;&quot;&quot;
In comparison to the previous mcmc results shown below, it looks like the standardized
analysis was actually pretty similar. But we can have greater confidence after
using the standardized analysis because we got a better posterior sample.
&quot;&quot;&quot;</span>

nbCode:
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b0: &quot;</span>, meanB0
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean b1: &quot;</span>, meanB1
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Mean sd: &quot;</span>, meanSd

  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b0: &quot;</span>, b0EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b0EtiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti b1: &quot;</span>, b1EtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b1EtiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Eti sd: &quot;</span>, sdEtiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, sdEtiMax.round(<span class="hljs-number">3</span>)

  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b0: &quot;</span>, b0HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b0HdiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi b1: &quot;</span>, b1HdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, b1HdiMax.round(<span class="hljs-number">3</span>)
  <span class="hljs-keyword">echo</span> <span class="hljs-string">&quot;Hdi sd: &quot;</span>, sdHdiMin.round(<span class="hljs-number">3</span>), <span class="hljs-string">&quot; - &quot;</span>, sdHdiMax.round(<span class="hljs-number">3</span>)

nbText: <span class="hljs-string">md&quot;&quot;&quot;
# Final Note
We could have simply and more efficiently done all of this using least squares regression.
However the Bayesian approach allows us to very easily and intuitively express
uncertainty about our estimates and can be easily extended to much more complex
models for which there are not such simple solutions. We could also incorporate prior
knowledge or assumptions in a way not possible with frequentist approaches.
The exercise above provides a starting point for this and shows just how easy it is to do with Nim!
&quot;&quot;&quot;</span>

nbSave</code></pre>
</section><script>
function toggleSourceDisplay() {
  var btn = document.getElementById("show")
  var source = document.getElementById("source");
  if (btn.innerHTML=="Show Source") {
    btn.innerHTML = "Hide Source";
    source.style.display = "block";
  } else {
    btn.innerHTML = "Show Source";
    source.style.display = "none";
  }
}
</script></body>
</html>